"use strict";(self.webpackChunksuperbench_website=self.webpackChunksuperbench_website||[]).push([[9075],{5680:(e,t,a)=>{a.d(t,{xA:()=>d,yg:()=>y});var n=a(6540);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function g(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var m=n.createContext({}),o=function(e){var t=n.useContext(m),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=o(e.components);return n.createElement(m.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,m=e.parentName,d=g(e,["components","mdxType","originalType","parentName"]),u=o(a),y=r,s=u["".concat(m,".").concat(y)]||u[y]||p[y]||l;return a?n.createElement(s,i(i({ref:t},d),{},{components:a})):n.createElement(s,i({ref:t},d))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=u;var g={};for(var m in t)hasOwnProperty.call(t,m)&&(g[m]=t[m]);g.originalType=e,g.mdxType="string"==typeof e?e:r,i[1]=g;for(var o=2;o<l;o++)i[o]=a[o];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},2444:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>m,default:()=>u,frontMatter:()=>g,metadata:()=>o,toc:()=>d});var n=a(8168),r=a(8587),l=(a(6540),a(5680)),i=["components"],g={id:"micro-benchmarks"},m="Micro Benchmarks",o={unversionedId:"user-tutorial/benchmarks/micro-benchmarks",id:"user-tutorial/benchmarks/micro-benchmarks",isDocsHomePage:!1,title:"Micro Benchmarks",description:"Computation Benchmarks",source:"@site/../docs/user-tutorial/benchmarks/micro-benchmarks.md",sourceDirName:"user-tutorial/benchmarks",slug:"/user-tutorial/benchmarks/micro-benchmarks",permalink:"/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks",editUrl:"https://github.com/microsoft/superbenchmark/edit/main/website/../docs/user-tutorial/benchmarks/micro-benchmarks.md",version:"current",frontMatter:{id:"micro-benchmarks"},sidebar:"docs",previous:{title:"Run SuperBench",permalink:"/superbenchmark/docs/getting-started/run-superbench"},next:{title:"Model Benchmarks",permalink:"/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks"}},d=[{value:"Computation Benchmarks",id:"computation-benchmarks",children:[{value:"<code>kernel-launch</code>",id:"kernel-launch",children:[]},{value:"<code>gemm-flops</code>",id:"gemm-flops",children:[]},{value:"<code>matmul</code>",id:"matmul",children:[]},{value:"<code>cublaslt-gemm</code> / <code>hipblaslt-gemm</code>",id:"cublaslt-gemm--hipblaslt-gemm",children:[]},{value:"<code>cublas-function</code>",id:"cublas-function",children:[]},{value:"<code>cudnn-function</code>",id:"cudnn-function",children:[]},{value:"<code>tensorrt-inference</code>",id:"tensorrt-inference",children:[]},{value:"<code>ort-inference</code>",id:"ort-inference",children:[]},{value:"<code>gpu-burn</code>",id:"gpu-burn",children:[]},{value:"<code>cpu-hpl</code>",id:"cpu-hpl",children:[]},{value:"<code>cpu-stream</code>",id:"cpu-stream",children:[]}]},{value:"Communication Benchmarks",id:"communication-benchmarks",children:[{value:"<code>cpu-memory-bw-latency</code>",id:"cpu-memory-bw-latency",children:[]},{value:"<code>mem-bw</code>",id:"mem-bw",children:[]},{value:"<code>gpu-copy-bw</code>",id:"gpu-copy-bw",children:[]},{value:"<code>ib-loopback</code>",id:"ib-loopback",children:[]},{value:"<code>nccl-bw</code> / <code>rccl-bw</code>",id:"nccl-bw--rccl-bw",children:[]},{value:"<code>tcp-connectivity</code>",id:"tcp-connectivity",children:[]},{value:"<code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code>",id:"gpcnet-network-test--gpcnet-network-load-test",children:[]},{value:"<code>ib-traffic</code>",id:"ib-traffic",children:[]},{value:"<code>nvbandwidth</code>",id:"nvbandwidth",children:[]}]},{value:"Computation-communication Benchmarks",id:"computation-communication-benchmarks",children:[{value:"<code>computation-communication-overlap</code>",id:"computation-communication-overlap",children:[]},{value:"<code>sharding-matmul</code>",id:"sharding-matmul",children:[]},{value:"<code>dist-inference</code>",id:"dist-inference",children:[]}]},{value:"Storage Benchmarks",id:"storage-benchmarks",children:[{value:"<code>disk-benchmark</code>",id:"disk-benchmark",children:[]}]}],p={toc:d};function u(e){var t=e.components,a=(0,r.A)(e,i);return(0,l.yg)("wrapper",(0,n.A)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,l.yg)("h1",{id:"micro-benchmarks"},"Micro Benchmarks"),(0,l.yg)("h2",{id:"computation-benchmarks"},"Computation Benchmarks"),(0,l.yg)("h3",{id:"kernel-launch"},(0,l.yg)("inlineCode",{parentName:"h3"},"kernel-launch")),(0,l.yg)("h4",{id:"introduction"},"Introduction"),(0,l.yg)("p",null,"Measure GPU kernel launch latency,\nwhich is defined as the time range from the beginning of the launch API call to the beginning of the kernel execution."),(0,l.yg)("h4",{id:"metrics"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"kernel-launch/event_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Launch latency measured in GPU time.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"kernel-launch/wall_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Launch latency measured in CPU time.")))),(0,l.yg)("h3",{id:"gemm-flops"},(0,l.yg)("inlineCode",{parentName:"h3"},"gemm-flops")),(0,l.yg)("h4",{id:"introduction-1"},"Introduction"),(0,l.yg)("p",null,"Measure the GPU GEMM FLOPS for different float and int data types, with or without Tensor Core (XDLOPS),\nperformed by NVIDIA ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/NVIDIA/cutlass/tree/ccb697bac77fcc898e9c897b2c90aa5b60ac72fb"},"cutlass"),"\nor AMD ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rocBLAS/tree/develop/clients/benchmarks"},"rocblas-bench"),"."),(0,l.yg)("h4",{id:"metrics-1"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp64_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp32_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float32 peak FLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp16_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp64_tc_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/tf32_tc_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp16_tc_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/bf16_tc_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/int8_tc_iops"),(0,l.yg)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/int4_tc_iops"),(0,l.yg)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM int4 peak IOPS with NVIDIA Tensor Core.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp32_xdlops_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with AMD XDLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/fp16_xdlops_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with AMD XDLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/bf16_xdlops_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with AMD XDLOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gemm-flops/int8_xdlops_iops"),(0,l.yg)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with AMD XDLOPS.")))),(0,l.yg)("h3",{id:"matmul"},(0,l.yg)("inlineCode",{parentName:"h3"},"matmul")),(0,l.yg)("h4",{id:"introduction-2"},"Introduction"),(0,l.yg)("p",null,"Large scale matmul operation using ",(0,l.yg)("inlineCode",{parentName:"p"},"torch.matmul")," with one GPU."),(0,l.yg)("h4",{id:"metrics-2"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-matmul/nosharding_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time of pure matmul operation.")))),(0,l.yg)("h3",{id:"cublaslt-gemm--hipblaslt-gemm"},(0,l.yg)("inlineCode",{parentName:"h3"},"cublaslt-gemm")," / ",(0,l.yg)("inlineCode",{parentName:"h3"},"hipblaslt-gemm")),(0,l.yg)("h4",{id:"introduction-3"},"Introduction"),(0,l.yg)("p",null,"Measure the GEMM performance of ",(0,l.yg)("a",{parentName:"p",href:"https://docs.nvidia.com/cuda/cublas/#cublasltmatmul"},(0,l.yg)("inlineCode",{parentName:"a"},"cublasLtMatmul"))," or ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/ROCm/hipBLASLt/blob/develop/clients/benchmarks/README.md"},(0,l.yg)("inlineCode",{parentName:"a"},"hipblasLt-bench")),"."),(0,l.yg)("h4",{id:"metrics-3"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cublaslt-gemm/${dtype}","_","${batch}","_","${m}","_","${n}","_","${k}_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (TFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"TFLOPS of measured GEMM kernel.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"hipblaslt-gemm/${dtype}","_","${batch}","_","${m}","_","${n}","_","${k}_flops"),(0,l.yg)("td",{parentName:"tr",align:null},"FLOPS (TFLOPS)"),(0,l.yg)("td",{parentName:"tr",align:null},"TFLOPS of measured GEMM kernel.")))),(0,l.yg)("h3",{id:"cublas-function"},(0,l.yg)("inlineCode",{parentName:"h3"},"cublas-function")),(0,l.yg)("h4",{id:"introduction-4"},"Introduction"),(0,l.yg)("p",null,"Measure the performance of most common Nvidia cuBLAS functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2."),(0,l.yg)("p",null,"The supported functions for cuBLAS are as follows:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"cublasSgemm"),(0,l.yg)("li",{parentName:"ul"},"cublasSgemmStridedBatched"),(0,l.yg)("li",{parentName:"ul"},"cublasGemmStridedBatchedEx"),(0,l.yg)("li",{parentName:"ul"},"cublasGemmEx"),(0,l.yg)("li",{parentName:"ul"},"cublasCgemm3mStridedBatched"),(0,l.yg)("li",{parentName:"ul"},"cublasCgemm")),(0,l.yg)("h4",{id:"metrics-4"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean time to execute the cublas function with the parameters.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_correctness"),(0,l.yg)("td",{parentName:"tr",align:null}),(0,l.yg)("td",{parentName:"tr",align:null},"Whether the calculation results of executing the cublas function with the parameters pass the correctness check if enable correctness check.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_error"),(0,l.yg)("td",{parentName:"tr",align:null}),(0,l.yg)("td",{parentName:"tr",align:null},"The error ratio of the calculation results of executing the cublas function with the parameters if enable correctness check.")))),(0,l.yg)("h3",{id:"cudnn-function"},(0,l.yg)("inlineCode",{parentName:"h3"},"cudnn-function")),(0,l.yg)("h4",{id:"introduction-5"},"Introduction"),(0,l.yg)("p",null,"Measure the performance of most common Nvidia cuDNN functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2."),(0,l.yg)("p",null,"The supported functions for cuDNN are as follows:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"cudnnConvolutionBackwardFilter"),(0,l.yg)("li",{parentName:"ul"},"cudnnConvolutionBackwardData"),(0,l.yg)("li",{parentName:"ul"},"cudnnConvolutionForward")),(0,l.yg)("h4",{id:"metrics-5"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cudnn-function/name","_","${function_name}","_","${parameters}_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean time to execute the cudnn function with the parameters.")))),(0,l.yg)("h3",{id:"tensorrt-inference"},(0,l.yg)("inlineCode",{parentName:"h3"},"tensorrt-inference")),(0,l.yg)("h4",{id:"introduction-6"},"Introduction"),(0,l.yg)("p",null,"Inference PyTorch/ONNX models on NVIDIA GPUs with ",(0,l.yg)("a",{parentName:"p",href:"https://developer.nvidia.com/tensorrt"},"TensorRT"),"."),(0,l.yg)("p",null,"Currently the following models are supported:"),(0,l.yg)("blockquote",null,(0,l.yg)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19\nlstm, bert-base, bert-large, gpt2-small")),(0,l.yg)("blockquote",null,(0,l.yg)("p",{parentName:"blockquote"},"Do not support large models like ",(0,l.yg)("inlineCode",{parentName:"p"},"gpt2-large")," currently because models larger than 2GB (maximum protobuf size) cannot be exported in one ONNX file.")),(0,l.yg)("h4",{id:"metrics-6"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_mean"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean GPU latency to execute the kernels for a query.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_99"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The 99th percentile GPU latency to execute the kernels for a query.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_mean"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_99"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The 99th percentile H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_mean"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean duration from when the H2D of a query is called to when the D2H of the same query is completed.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_99"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The P99 duration from when the H2D of a query is called to when the D2H of the same query is completed.")))),(0,l.yg)("h3",{id:"ort-inference"},(0,l.yg)("inlineCode",{parentName:"h3"},"ort-inference")),(0,l.yg)("h4",{id:"introduction-7"},"Introduction"),(0,l.yg)("p",null,"Inference performance of the torchvision models using ONNXRuntime. Currently the following models are supported:"),(0,l.yg)("blockquote",null,(0,l.yg)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19")),(0,l.yg)("p",null,"The supported percentiles are 50, 90, 95, 99, and 99.9."),(0,l.yg)("h4",{id:"metrics-7"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ort-inference/{precision}_{model}_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The mean latency to execute one batch of inference.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ort-inference/{precision}",(0,l.yg)("em",{parentName:"td"},"{model}_time"),"{percentile}"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"The {percentile}th percentile latency to execute one batch of inference.")))),(0,l.yg)("h3",{id:"gpu-burn"},(0,l.yg)("inlineCode",{parentName:"h3"},"gpu-burn")),(0,l.yg)("h4",{id:"introduction-8"},"Introduction"),(0,l.yg)("p",null,"Multi-GPU CUDA stress test for GPU compute and memory utilization, performed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/wilicc/gpu-burn"},"gpu-burn"),".\nSupports the use of double unit types and the use of tensor cores."),(0,l.yg)("h4",{id:"metrics-8"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu-burn/time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The runtime for gpu-burn test.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu-burn/gpu_","[0-9]","_pass"),(0,l.yg)("td",{parentName:"tr",align:null},"yes/no"),(0,l.yg)("td",{parentName:"tr",align:null},"The result of the gpu-burn test for each GPU (1: yes, 0: no).")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu-burn/abort"),(0,l.yg)("td",{parentName:"tr",align:null},"yes/no"),(0,l.yg)("td",{parentName:"tr",align:null},"Whether or not GPU-burn test aborted before returning GPU results (1: yes, 0: no).")))),(0,l.yg)("h3",{id:"cpu-hpl"},(0,l.yg)("inlineCode",{parentName:"h3"},"cpu-hpl")),(0,l.yg)("h4",{id:"introduction-9"},"Introduction"),(0,l.yg)("p",null,"HPL or High Performance Computing Linpack evaluates compute bandwidth by solving dense linear systems in double precision arethmetic.\nPerformed by ",(0,l.yg)("a",{parentName:"p",href:"https://netlib.org/benchmark/hpl/"},"High-Performance Linpack Benchmark for Distributed-Memory Computers")),(0,l.yg)("h4",{id:"metrics-9"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-hpl/tests_pass"),(0,l.yg)("td",{parentName:"tr",align:null}),(0,l.yg)("td",{parentName:"tr",align:null},"HPL completed running and correctness test has passed (1: pass, 0: fail).")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-hpl/throughput"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GFlops)"),(0,l.yg)("td",{parentName:"tr",align:null},"Compute bandwidth.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-hpl/time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time elapsed during HPL run.")))),(0,l.yg)("h3",{id:"cpu-stream"},(0,l.yg)("inlineCode",{parentName:"h3"},"cpu-stream")),(0,l.yg)("h4",{id:"introduction-10"},"Introduction"),(0,l.yg)("p",null,"Measure of memory bandwidth and computation rate for simple vector kernels.\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://www.cs.virginia.edu/stream/ref.html"},"University of Virginia STREAM benchmark"),"."),(0,l.yg)("h4",{id:"metrics-10"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-stream/threads"),(0,l.yg)("td",{parentName:"tr",align:null}),(0,l.yg)("td",{parentName:"tr",align:null},"Number of threads used for the test. Determined by core count.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","throughput"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Memory throughput of designated kerel operation.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_avg"),(0,l.yg)("td",{parentName:"tr",align:null},"time (s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Average elapsed times over all iterations.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_min"),(0,l.yg)("td",{parentName:"tr",align:null},"time (s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Minimum elapsed times over all iterations.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_max"),(0,l.yg)("td",{parentName:"tr",align:null},"time (s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Maximum elapsed times over all iterations.")))),(0,l.yg)("h2",{id:"communication-benchmarks"},"Communication Benchmarks"),(0,l.yg)("h3",{id:"cpu-memory-bw-latency"},(0,l.yg)("inlineCode",{parentName:"h3"},"cpu-memory-bw-latency")),(0,l.yg)("h4",{id:"introduction-11"},"Introduction"),(0,l.yg)("p",null,"Measure the memory copy bandwidth and latency across different CPU NUMA nodes.\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html"},"Intel MLC Tool"),"."),(0,l.yg)("h4",{id:"metrics-11"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","bandwidth","_","matrix","_","numa","_","[0-9]","+","_","[0-9]","+","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Former NUMA to latter NUMA memory bandwidth.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","bandwidth","_","matrix","_","numa","_","[0-9]","+","_","[0-9]","+","_","lat"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Former NUMA to latter NUMA memory latency.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","all","_","reads","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, full read.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","3_1","_","reads-writes","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 3 : 1.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","2_1","_","reads-writes","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 2 : 1.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","1_1","_","reads-writes","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 1 : 1.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","stream-triad","_","like","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, with stream-triad like pattern.")))),(0,l.yg)("h3",{id:"mem-bw"},(0,l.yg)("inlineCode",{parentName:"h3"},"mem-bw")),(0,l.yg)("h4",{id:"introduction-12"},"Introduction"),(0,l.yg)("p",null,"Measure the memory copy bandwidth across PCI-e and memory copy bandwidth between GPUs,\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest"},"NVIDIA"),"\nor ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/ROCm-Developer-Tools/HIP/tree/master/samples/1_Utils/hipBusBandwidth"},"AMD")," bandwidth test tool."),(0,l.yg)("h4",{id:"metrics-12"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"mem-bw/h2d_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Host to device copy bandwidth.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"mem-bw/d2h_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Device to host copy bandwidth.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"mem-bw/d2d_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Device to device copy bandwidth.")))),(0,l.yg)("h3",{id:"gpu-copy-bw"},(0,l.yg)("inlineCode",{parentName:"h3"},"gpu-copy-bw")),(0,l.yg)("p",null,"Measure the memory copy bandwidth performed by GPU SM/DMA engine, including device-to-host, host-to-device and device-to-device.\nFor measurements of peer-to-peer communication performance between AMD GPUs, GPU memory buffers are allocated in ",(0,l.yg)("inlineCode",{parentName:"p"},"hipDeviceMallocUncached")," (previous ",(0,l.yg)("inlineCode",{parentName:"p"},"hipDeviceMallocFinegrained"),") mode to maximize performance."),(0,l.yg)("h4",{id:"metrics-13"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu","_","to","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","cpu","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU writing one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading or writing self's memory using DMA engine or GPU SM.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","[0-9]","+","_","(read","|","write)","_","by","_","(sm","|","dma)","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading or writing peer GPU's memory using DMA engine or GPU SM with peer communication enabled.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"cpu","_","and","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","cpu","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"Same as above, but generated by --dtoh --bidirectional.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing self's memory using DMA engine or GPU SM.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","gpu","[0-9]","+","_","(read","|","write)","_","by","_","(sm","|","dma)","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing peer GPU's memory using DMA engine or GPU SM with peer communication enabled.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","_","all","_","write","_","by","_","sm","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU writing all peer GPUs' memory using GPU SM with peer communication enabled.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","_","all","_","to","_","gpu","[0-9]","+","_","write","_","by","_","sm","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of all peer GPUs writing one GPU's memory using GPU SM with peer communication enabled.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpu","_","all","_","to","_","gpu","_","all","_","write","_","by","_","sm","_","bw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of all peer GPUs writing all peer GPUs' memory using GPU SM with peer communication enabled.")))),(0,l.yg)("h3",{id:"ib-loopback"},(0,l.yg)("inlineCode",{parentName:"h3"},"ib-loopback")),(0,l.yg)("h4",{id:"introduction-13"},"Introduction"),(0,l.yg)("p",null,"Measure the InfiniBand loopback verbs bandwidth, performed by\n",(0,l.yg)("a",{parentName:"p",href:"https://github.com/linux-rdma/perftest/tree/7504ce48ac396a02f4d00de359257b2cb8458f06"},"OFED performance tests"),"."),(0,l.yg)("h4",{id:"metrics-14"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.yg)("em",{parentName:"td"},"write_bw"),"${msg_size}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"InfiniBand loopback write bandwidth with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.yg)("em",{parentName:"td"},"read_bw"),"${msg_size}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"InfiniBand loopback read bandwidth with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.yg)("em",{parentName:"td"},"send_bw"),"${msg_size}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"InfiniBand loopback send bandwidth with given message size.")))),(0,l.yg)("h3",{id:"nccl-bw--rccl-bw"},(0,l.yg)("inlineCode",{parentName:"h3"},"nccl-bw")," / ",(0,l.yg)("inlineCode",{parentName:"h3"},"rccl-bw")),(0,l.yg)("h4",{id:"introduction-14"},"Introduction"),(0,l.yg)("p",null,"Measure the performance of NCCL/RCCL operations under multi nodes' traffic pattern,\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/NVIDIA/nccl-tests/tree/44df0bf010dcc95e840ca0fb7466c67cff3f1f0f"},"nccl-tests"),"\nor ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rccl-tests/tree/dc1ad4853d7ec738387d42a75a58a98d7af00c7b"},"rccl-tests"),".\nSupport the following operations currently: allreduce, allgather, broadcast, reduce, reducescatter, alltoall.\nSupport both in-place and out-of-place measurements."),(0,l.yg)("p",null,"Support the following traffic patterns:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"all-nodes"),", validate the NCCL/RCCL performance across all VM nodes simultaneously."),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"pair-wise"),", validate the NCCL/RCCL performance across VM pairs with all possible combinations in parallel."),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"k-batch"),", validate the NCCL/RCCL performance across VM groups with a specified batch scale."),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"topo-aware"),", validate the NCCL/RCCL performance across VM pairs with different distances/hops as a quick test.")),(0,l.yg)("h4",{id:"metrics-15"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"NCCL operation lantency with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_algbw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"NCCL operation algorithm bandwidth with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_busbw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"NCCL operation bus bandwidth with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"RCCL operation lantency with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_algbw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"RCCL operation algorithm bandwidth with given message size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_busbw"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"RCCL operation bus bandwidth with given message size.")))),(0,l.yg)("p",null,"If mpi mode is enable and traffic pattern is specified, the metrics pattern will change to ",(0,l.yg)("inlineCode",{parentName:"p"},"nccl-bw/${operation}_${serial_index)_${parallel_index):${msg_size}_time")),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"serial_index")," represents the serial index of the host group in serial."),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"parallel_index")," represents the parallel index of the host list in parallel.")),(0,l.yg)("h3",{id:"tcp-connectivity"},(0,l.yg)("inlineCode",{parentName:"h3"},"tcp-connectivity")),(0,l.yg)("h4",{id:"introduction-15"},"Introduction"),(0,l.yg)("p",null,"Test the TCP connectivity between current node and nodes in the hostfile,\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/zhengxiaowai/tcping"},"tcping")),(0,l.yg)("h4",{id:"metrics-16"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Metrics"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_successed_count"),(0,l.yg)("td",{parentName:"tr",align:null},"count"),(0,l.yg)("td",{parentName:"tr",align:null},"successed times of tcp connections between current node and other nodes")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_failed_count"),(0,l.yg)("td",{parentName:"tr",align:null},"count"),(0,l.yg)("td",{parentName:"tr",align:null},"failed times of tcp connections between current node and other nodes")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_success_rate"),(0,l.yg)("td",{parentName:"tr",align:null}),(0,l.yg)("td",{parentName:"tr",align:null},"success rate (successed/total) of tcp connection between current node and other nodes")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_min"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"mininum latency of tcp connections between current node and other nodes")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_max"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"maximum latency of tcp connections between current node and other nodes")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_avg"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"average latency of tcp connections between current node and other nodes")))),(0,l.yg)("h3",{id:"gpcnet-network-test--gpcnet-network-load-test"},(0,l.yg)("inlineCode",{parentName:"h3"},"gpcnet-network-test")," / ",(0,l.yg)("inlineCode",{parentName:"h3"},"gpcnet-network-load-test")),(0,l.yg)("h4",{id:"introduction-16"},"Introduction"),(0,l.yg)("p",null,"Distributed test, test the global network performance and congestion,\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/netbench/GPCNET"},"GPCNET")),(0,l.yg)("p",null,"gpcnet-network-test: Full system network tests in random and natural ring, alltoall and allreduce, at least 2 nodes"),(0,l.yg)("p",null,"gpcnet-network-load-test: Select full system network tests run with four congestors to measure network congestion or contention, at least 10 nodes"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"supporting network tests: RR Two-sided Lat (8 B), RR Get Lat (8 B), RR Two-sided BW (131072 B), RR Put BW (131072 B), RR Two-sided BW+Sync (131072 B), Nat Two-sided BW (131072 B), Multiple Allreduce (8 B), Multiple Alltoall (4096 B)"),(0,l.yg)("li",{parentName:"ul"},"supporting congestors: Alltoall (4096 B), Two-sided Incast (4096 B), Put Incast (4096 B), Get Bcast (4096 B)")),(0,l.yg)("h4",{id:"metrics-17"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Metrics"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.yg)("em",{parentName:"td"},"two-sided_lat"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side latency' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.yg)("em",{parentName:"td"},"two-sided+sync_bw"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.yg)("td",{parentName:"tr",align:null},"fstatistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side bandwidth with barrier' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,l.yg)("em",{parentName:"td"},"allreduce_time"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'multiple allreduce bandwidth' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.yg)("em",{parentName:"td"},"get_lat"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR GetLat (8 B)' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.yg)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR Two-sidedBW (131072 B)' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/nat",(0,l.yg)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Nat Two-sidedBW (131072 B)' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,l.yg)("em",{parentName:"td"},"alltoall_bw"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.yg)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Multiple Alltoall (4096 B)' for network testing")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,l.yg)("em",{parentName:"td"},"two-sided_lat_x"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.yg)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,l.yg)("em",{parentName:"td"},"two-sided+sync_bw_x"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.yg)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/multiple",(0,l.yg)("em",{parentName:"td"},"allreduce_x"),"${stat}"),(0,l.yg)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.yg)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")))),(0,l.yg)("h3",{id:"ib-traffic"},(0,l.yg)("inlineCode",{parentName:"h3"},"ib-traffic")),(0,l.yg)("h4",{id:"introduction-17"},"Introduction"),(0,l.yg)("p",null,"Measure the InfiniBand performance under multi nodes' traffic pattern."),(0,l.yg)("p",null,"The direction between client and server can be 'cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu'."),(0,l.yg)("p",null,"The traffic pattern is defined in a config file, which is pre-defined for one-to-many, many-to-one and all-to-all patterns.\nEach row in the config is one round, and all pairs of nodes in a row run ib command simultaneously."),(0,l.yg)("p",null,"Besides the above three patterns, ib-traffic also supports topology-aware traffic pattern. To run ib-traffic with topology-aware\npattern, the user needs to specify 3 required (and 2 optional) parameters in YAML config file:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"--pattern\t","\u2003",(0,l.yg)("strong",{parentName:"li"},"topo-aware")),(0,l.yg)("li",{parentName:"ul"},"--ibstat\t","\u2003",(0,l.yg)("strong",{parentName:"li"},"path to ibstat output")),(0,l.yg)("li",{parentName:"ul"},"--ibnetdiscover\t","\u2003",(0,l.yg)("strong",{parentName:"li"},"path to ibnetdiscover output")),(0,l.yg)("li",{parentName:"ul"},"--min_dist\t","\u2003",(0,l.yg)("strong",{parentName:"li"},"minimum distance of VM pairs (optional, default 2)")),(0,l.yg)("li",{parentName:"ul"},"--max_dist\t","\u2003",(0,l.yg)("strong",{parentName:"li"},"maximum distance of VM pairs (optional, default 6)"))),(0,l.yg)("p",null,"Each row in the config file has all VM pairs with a fixed distance (#hops). That's by default, 1st, 2nd, 3rd row has all VM pairs\nwith topology distance of 2, 4, 6, respectively."),(0,l.yg)("h4",{id:"metrics-18"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Metrics"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ib-traffic/ib","_","write","_","bw","_","${msg_size}","_","${direction}","_","${line}","_","${pair}:${server}","_","${client}"),(0,l.yg)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.yg)("td",{parentName:"tr",align:null},"The max bandwidth of perftest (ib_write_bw, ib_send_bw, ib_read_bw) using ${msg_size} with ${direction}('cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu') run between the ${pair}",(0,l.yg)("sup",null,"th")," node pair in the ${line}",(0,l.yg)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"ib-traffic/ib","_","write","_","lat","_","${msg_size}","_","${direction}","_","${line}","_","${pair}:${server}","_","${client}"),(0,l.yg)("td",{parentName:"tr",align:null},"time (us)"),(0,l.yg)("td",{parentName:"tr",align:null},"The max latency of perftest (ib_write_lat, ib_send_lat, ib_read_lat) using ${msg_size} with ${direction}('cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu') run between the ${pair}",(0,l.yg)("sup",null,"th")," node pair in the ${line}",(0,l.yg)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client.")))),(0,l.yg)("h3",{id:"nvbandwidth"},(0,l.yg)("inlineCode",{parentName:"h3"},"nvbandwidth")),(0,l.yg)("h4",{id:"introduction-18"},"Introduction"),(0,l.yg)("p",null,"Measures bandwidth and latency for various memcpy patterns across different links using copy engine or kernel copy methods,\nperformed by ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/NVIDIA/nvbandwidth"},"nvbandwidth")),(0,l.yg)("h4",{id:"metrics-19"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Metrics"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Host to device CE memcpy using cuMemcpyAsync")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Device to host CE memcpy using cuMemcpyAsync")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_bidirectional_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A host to device copy is measured while a device to host copy is run simultaneously. Only the host to device copy bandwidth is reported.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_bidirectional_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_bidirectional_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A device to host copy is measured while a host to device copy is run simultaneously. Only the device to host copy bandwidth is reported.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_bidirectional_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_read_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between each pair of accessible peers. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_read_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_write_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between each pair of accessible peers. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_write_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_read_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between each pair of accessible peers. A copy in the opposite direction of the measured copy is run simultaneously but not measured. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_read_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_write_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between each pair of accessible peers. A copy in the opposite direction of the measured copy is run simultaneously but not measured. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_write_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between a single device and the host while simultaneously running copies from all other devices to the host.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_bidirectional_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A device to host copy is measured while a host to device copy is run simultaneously. Only the device to host copy bandwidth is reported. All other devices generate simultaneous host to device and device to host interfering traffic.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_bidirectional_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of cuMemcpyAsync between the host to a single device while simultaneously running copies from the host to all other devices.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_bidirectional_memcpy_ce_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A host to device copy is measured while a device to host copy is run simultaneously. Only the host to device copy bandwidth is reported. All other devices generate simultaneous host to device and device to host interfering traffic.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_bidirectional_memcpy_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_write_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from all accessible peers to a single device, for each device. Bandwidth is reported as the total inbound bandwidth for each device. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_write_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_read_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from all accessible peers to a single device, for each device. Bandwidth is reported as the total outbound bandwidth for each device. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_read_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_write_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from a single device to all accessible peers, for each device. Bandwidth is reported as the total outbound bandwidth for each device. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_write_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_read_ce_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from a single device to all accessible peers, for each device. Bandwidth is reported as the total inbound bandwidth for each device. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_read_ce_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Host to device SM memcpy using a copy kernel")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_device_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Device to host SM memcpy using a copy kernel")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_host_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_read_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between each pair of accessible peers. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_read_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_write_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between each pair of accessible peers. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_memcpy_write_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_read_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between each pair of accessible peers. Copies are run in both directions between each pair, and the sum is reported. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_read_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_write_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between each pair of accessible peers. Copies are run in both directions between each pair, and the sum is reported. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_bidirectional_memcpy_write_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between a single device and the host while simultaneously running copies from all other devices to the host.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_bidirectional_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A device to host bandwidth of a copy kernel is measured while a host to device copy is run simultaneously. Only the device to host copy bandwidth is reported. All other devices generate simultaneous host to device and device to host interfering traffic using copy kernels.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_host_bidirectional_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures bandwidth of a copy kernel between the host to a single device while simultaneously running copies from the host to all other devices.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_bidirectional_memcpy_sm_cpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"A host to device bandwidth of a copy kernel is measured while a device to host copy is run simultaneously. Only the host to device copy bandwidth is reported. All other devices generate simultaneous host to device and device to host interfering traffic using copy kernels.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_to_all_bidirectional_memcpy_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_write_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from all accessible peers to a single device, for each device. Bandwidth is reported as the total inbound bandwidth for each device. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_write_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_read_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from all accessible peers to a single device, for each device. Bandwidth is reported as the total outbound bandwidth for each device. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"all_to_one_read_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_write_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from a single device to all accessible peers, for each device. Bandwidth is reported as the total outbound bandwidth for each device. Write tests launch a copy from the target device to the peer using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_write_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_read_sm_gpu","[0-9]","_gpu","[0-9]","_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures the total bandwidth of copies from a single device to all accessible peers, for each device. Bandwidth is reported as the total inbound bandwidth for each device. Read tests launch a copy from the peer device to the target using the target's context.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"one_to_all_read_sm_sum_bw"),(0,l.yg)("td",{parentName:"tr",align:null},"GB/s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_device_latency_sm_cpu","[0-9]","_gpu","[0-9]","_lat"),(0,l.yg)("td",{parentName:"tr",align:null},"\xb5s"),(0,l.yg)("td",{parentName:"tr",align:null},"Host - device SM copy latency using a ptr chase kernel")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"host_device_latency_sm_sum_lat"),(0,l.yg)("td",{parentName:"tr",align:null},"\xb5s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_latency_sm_gpu","[0-9]","_gpu","[0-9]","_lat"),(0,l.yg)("td",{parentName:"tr",align:null},"\xb5s"),(0,l.yg)("td",{parentName:"tr",align:null},"Measures latency of a pointer dereference operation between each pair of accessible peers. Memory is allocated on a GPU and is accessed by the peer GPU to determine latency.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"device_to_device_latency_sm_sum_lat"),(0,l.yg)("td",{parentName:"tr",align:null},"\xb5s"),(0,l.yg)("td",{parentName:"tr",align:null},"Sum of the output matrix")))),(0,l.yg)("h2",{id:"computation-communication-benchmarks"},"Computation-communication Benchmarks"),(0,l.yg)("h3",{id:"computation-communication-overlap"},(0,l.yg)("inlineCode",{parentName:"h3"},"computation-communication-overlap")),(0,l.yg)("h4",{id:"introduction-19"},"Introduction"),(0,l.yg)("p",null,"Test the performance of single node when communication and computation overlap."),(0,l.yg)("h4",{id:"metrics-20"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/mul_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time of communication and mul kernel computation overlap.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/matmul_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time of communication and matmul kernel computation overlap.")))),(0,l.yg)("h4",{id:""}),(0,l.yg)("h3",{id:"sharding-matmul"},(0,l.yg)("inlineCode",{parentName:"h3"},"sharding-matmul")),(0,l.yg)("h4",{id:"introduction-20"},"Introduction"),(0,l.yg)("p",null,"Test the performance of large scale matmul operation with multiple GPUs:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},"allreduce: Each GPU will calculate part of the MM calculation, and use AllReduce to merge all data into one tensor."),(0,l.yg)("li",{parentName:"ul"},"allgather: Each GPU will calculate part of the MM calculation, and use AllGather + Concat to merge all data into one tensor.")),(0,l.yg)("h4",{id:"metrics-21"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allreduce_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time of sharding matmul using allreduce.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allgather_time"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Time of sharding matmul using allgather.")))),(0,l.yg)("h3",{id:"dist-inference"},(0,l.yg)("inlineCode",{parentName:"h3"},"dist-inference")),(0,l.yg)("h4",{id:"introduction-21"},"Introduction"),(0,l.yg)("p",null,"Test the performance of distributed model inference. Support both PyTorch implementation and cpp implementation."),(0,l.yg)("h4",{id:"metrics-22"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-dist-inference/step_times"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Average time of model inference runs.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"pytorch-dist-inference/step",(0,l.yg)("em",{parentName:"td"},"times"),"${percentile}"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.yg)("td",{parentName:"tr",align:null},"Tail (50,90,95,99,99.9) time of model inference runs.")))),(0,l.yg)("h2",{id:"storage-benchmarks"},"Storage Benchmarks"),(0,l.yg)("h3",{id:"disk-benchmark"},(0,l.yg)("inlineCode",{parentName:"h3"},"disk-benchmark")),(0,l.yg)("h4",{id:"introduction-22"},"Introduction"),(0,l.yg)("p",null,"Measure the disk performance through ",(0,l.yg)("a",{parentName:"p",href:"https://github.com/axboe/fio/tree/0313e938c9c8bb37d71dade239f1f5326677b079"},"FIO"),"."),(0,l.yg)("h4",{id:"metrics-23"},"Metrics"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Name"),(0,l.yg)("th",{parentName:"tr",align:null},"Unit"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_bs"),(0,l.yg)("td",{parentName:"tr",align:null},"size (bytes)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write block size.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_iops"),(0,l.yg)("td",{parentName:"tr",align:null},"IOPS"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write read IOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_95.0"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write read latency in 95.0 percentile.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.0"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.0 percentile.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.9"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.9 percentile.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_iops"),(0,l.yg)("td",{parentName:"tr",align:null},"IOPS"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write write IOPS.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_95.0"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write write latency in 95.0 percentile.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.0"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.0 percentile.")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.9"),(0,l.yg)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.yg)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.9 percentile.")))))}u.isMDXComponent=!0}}]);
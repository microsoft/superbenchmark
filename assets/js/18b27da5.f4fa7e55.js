"use strict";(self.webpackChunksuperbench_website=self.webpackChunksuperbench_website||[]).push([[8712],{3905:function(t,e,n){n.d(e,{Zo:function(){return p},kt:function(){return s}});var a=n(7294);function r(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function l(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,a)}return n}function i(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?l(Object(n),!0).forEach((function(e){r(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function m(t,e){if(null==t)return{};var n,a,r=function(t,e){if(null==t)return{};var n,a,r={},l=Object.keys(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||(r[n]=t[n]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(a=0;a<l.length;a++)n=l[a],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(r[n]=t[n])}return r}var o=a.createContext({}),d=function(t){var e=a.useContext(o),n=e;return t&&(n="function"==typeof t?t(e):i(i({},e),t)),n},p=function(t){var e=d(t.components);return a.createElement(o.Provider,{value:e},t.children)},u={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},c=a.forwardRef((function(t,e){var n=t.components,r=t.mdxType,l=t.originalType,o=t.parentName,p=m(t,["components","mdxType","originalType","parentName"]),c=d(n),s=r,k=c["".concat(o,".").concat(s)]||c[s]||u[s]||l;return n?a.createElement(k,i(i({ref:e},p),{},{components:n})):a.createElement(k,i({ref:e},p))}));function s(t,e){var n=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=n.length,i=new Array(l);i[0]=c;var m={};for(var o in e)hasOwnProperty.call(e,o)&&(m[o]=e[o]);m.originalType=t,m.mdxType="string"==typeof t?t:r,i[1]=m;for(var d=2;d<l;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},485:function(t,e,n){n.r(e),n.d(e,{contentTitle:function(){return o},default:function(){return c},frontMatter:function(){return m},metadata:function(){return d},toc:function(){return p}});var a=n(7462),r=n(3366),l=(n(7294),n(3905)),i=["components"],m={id:"micro-benchmarks"},o="Micro Benchmarks",d={unversionedId:"user-tutorial/benchmarks/micro-benchmarks",id:"user-tutorial/benchmarks/micro-benchmarks",isDocsHomePage:!1,title:"Micro Benchmarks",description:"Computation Benchmarks",source:"@site/../docs/user-tutorial/benchmarks/micro-benchmarks.md",sourceDirName:"user-tutorial/benchmarks",slug:"/user-tutorial/benchmarks/micro-benchmarks",permalink:"/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks",editUrl:"https://github.com/microsoft/superbenchmark/edit/main/website/../docs/user-tutorial/benchmarks/micro-benchmarks.md",version:"current",frontMatter:{id:"micro-benchmarks"},sidebar:"docs",previous:{title:"Run SuperBench",permalink:"/superbenchmark/docs/getting-started/run-superbench"},next:{title:"Model Benchmarks",permalink:"/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks"}},p=[{value:"Computation Benchmarks",id:"computation-benchmarks",children:[{value:"<code>kernel-launch</code>",id:"kernel-launch",children:[]},{value:"<code>gemm-flops</code>",id:"gemm-flops",children:[]},{value:"<code>matmul</code>",id:"matmul",children:[]},{value:"<code>cublaslt-gemm</code>",id:"cublaslt-gemm",children:[]},{value:"<code>cublas-function</code>",id:"cublas-function",children:[]},{value:"<code>cudnn-function</code>",id:"cudnn-function",children:[]},{value:"<code>tensorrt-inference</code>",id:"tensorrt-inference",children:[]},{value:"<code>ort-inference</code>",id:"ort-inference",children:[]},{value:"<code>gpu-burn</code>",id:"gpu-burn",children:[]},{value:"<code>cpu-hpl</code>",id:"cpu-hpl",children:[]},{value:"<code>cpu-stream</code>",id:"cpu-stream",children:[]}]},{value:"Communication Benchmarks",id:"communication-benchmarks",children:[{value:"<code>cpu-memory-bw-latency</code>",id:"cpu-memory-bw-latency",children:[]},{value:"<code>mem-bw</code>",id:"mem-bw",children:[]},{value:"<code>gpu-copy-bw</code>",id:"gpu-copy-bw",children:[]},{value:"<code>ib-loopback</code>",id:"ib-loopback",children:[]},{value:"<code>nccl-bw</code> / <code>rccl-bw</code>",id:"nccl-bw--rccl-bw",children:[]},{value:"<code>tcp-connectivity</code>",id:"tcp-connectivity",children:[]},{value:"<code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code>",id:"gpcnet-network-test--gpcnet-network-load-test",children:[]},{value:"<code>ib-traffic</code>",id:"ib-traffic",children:[]}]},{value:"Computation-communication Benchmarks",id:"computation-communication-benchmarks",children:[{value:"<code>computation-communication-overlap</code>",id:"computation-communication-overlap",children:[]},{value:"<code>sharding-matmul</code>",id:"sharding-matmul",children:[]},{value:"<code>dist-inference</code>",id:"dist-inference",children:[]}]},{value:"Storage Benchmarks",id:"storage-benchmarks",children:[{value:"<code>disk-benchmark</code>",id:"disk-benchmark",children:[]}]}],u={toc:p};function c(t){var e=t.components,n=(0,r.Z)(t,i);return(0,l.kt)("wrapper",(0,a.Z)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"micro-benchmarks"},"Micro Benchmarks"),(0,l.kt)("h2",{id:"computation-benchmarks"},"Computation Benchmarks"),(0,l.kt)("h3",{id:"kernel-launch"},(0,l.kt)("inlineCode",{parentName:"h3"},"kernel-launch")),(0,l.kt)("h4",{id:"introduction"},"Introduction"),(0,l.kt)("p",null,"Measure GPU kernel launch latency,\nwhich is defined as the time range from the beginning of the launch API call to the beginning of the kernel execution."),(0,l.kt)("h4",{id:"metrics"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"kernel-launch/event_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Launch latency measured in GPU time.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"kernel-launch/wall_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Launch latency measured in CPU time.")))),(0,l.kt)("h3",{id:"gemm-flops"},(0,l.kt)("inlineCode",{parentName:"h3"},"gemm-flops")),(0,l.kt)("h4",{id:"introduction-1"},"Introduction"),(0,l.kt)("p",null,"Measure the GPU GEMM FLOPS for different float and int data types, with or without Tensor Core (XDLOPS),\nperformed by NVIDIA ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/cutlass/tree/ccb697bac77fcc898e9c897b2c90aa5b60ac72fb"},"cutlass"),"\nor AMD ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rocBLAS/tree/develop/clients/benchmarks"},"rocblas-bench"),"."),(0,l.kt)("h4",{id:"metrics-1"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp64_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp32_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float32 peak FLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp64_tc_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float64 peak FLOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/tf32_tc_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_tc_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/bf16_tc_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/int8_tc_iops"),(0,l.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/int4_tc_iops"),(0,l.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM int4 peak IOPS with NVIDIA Tensor Core.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp32_xdlops_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM tensor-float32 peak FLOPS with AMD XDLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/fp16_xdlops_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM float16 peak FLOPS with AMD XDLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/bf16_xdlops_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (GFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM bfloat16 peak FLOPS with AMD XDLOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gemm-flops/int8_xdlops_iops"),(0,l.kt)("td",{parentName:"tr",align:null},"IOPS (GIOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"GEMM int8 peak IOPS with AMD XDLOPS.")))),(0,l.kt)("h3",{id:"matmul"},(0,l.kt)("inlineCode",{parentName:"h3"},"matmul")),(0,l.kt)("h4",{id:"introduction-2"},"Introduction"),(0,l.kt)("p",null,"Large scale matmul operation using ",(0,l.kt)("inlineCode",{parentName:"p"},"torch.matmul")," with one GPU."),(0,l.kt)("h4",{id:"metrics-2"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-matmul/nosharding_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time of pure matmul operation.")))),(0,l.kt)("h3",{id:"cublaslt-gemm"},(0,l.kt)("inlineCode",{parentName:"h3"},"cublaslt-gemm")),(0,l.kt)("h4",{id:"introduction-3"},"Introduction"),(0,l.kt)("p",null,"Measure the GEMM performance of ",(0,l.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/cuda/cublas/#cublasltmatmul"},(0,l.kt)("inlineCode",{parentName:"a"},"cublasLtMatmul")),"."),(0,l.kt)("h4",{id:"metrics-3"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cublaslt-gemm/${dtype}","_","${batch}","_","${m}","_","${n}","_","${k}_flops"),(0,l.kt)("td",{parentName:"tr",align:null},"FLOPS (TFLOPS)"),(0,l.kt)("td",{parentName:"tr",align:null},"TFLOPS of measured GEMM kernel.")))),(0,l.kt)("h3",{id:"cublas-function"},(0,l.kt)("inlineCode",{parentName:"h3"},"cublas-function")),(0,l.kt)("h4",{id:"introduction-4"},"Introduction"),(0,l.kt)("p",null,"Measure the performance of most common Nvidia cuBLAS functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2."),(0,l.kt)("p",null,"The supported functions for cuBLAS are as follows:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"cublasSgemm"),(0,l.kt)("li",{parentName:"ul"},"cublasSgemmStridedBatched"),(0,l.kt)("li",{parentName:"ul"},"cublasGemmStridedBatchedEx"),(0,l.kt)("li",{parentName:"ul"},"cublasGemmEx"),(0,l.kt)("li",{parentName:"ul"},"cublasCgemm3mStridedBatched"),(0,l.kt)("li",{parentName:"ul"},"cublasCgemm")),(0,l.kt)("h4",{id:"metrics-4"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean time to execute the cublas function with the parameters.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_correctness"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Whether the calculation results of executing the cublas function with the parameters pass the correctness check if enable correctness check.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cublas-function/name","_","${function_name}","_","${parameters}_error"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"The error ratio of the calculation results of executing the cublas function with the parameters if enable correctness check.")))),(0,l.kt)("h3",{id:"cudnn-function"},(0,l.kt)("inlineCode",{parentName:"h3"},"cudnn-function")),(0,l.kt)("h4",{id:"introduction-5"},"Introduction"),(0,l.kt)("p",null,"Measure the performance of most common Nvidia cuDNN functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2."),(0,l.kt)("p",null,"The supported functions for cuDNN are as follows:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"cudnnConvolutionBackwardFilter"),(0,l.kt)("li",{parentName:"ul"},"cudnnConvolutionBackwardData"),(0,l.kt)("li",{parentName:"ul"},"cudnnConvolutionForward")),(0,l.kt)("h4",{id:"metrics-5"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cudnn-function/name","_","${function_name}","_","${parameters}_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean time to execute the cudnn function with the parameters.")))),(0,l.kt)("h3",{id:"tensorrt-inference"},(0,l.kt)("inlineCode",{parentName:"h3"},"tensorrt-inference")),(0,l.kt)("h4",{id:"introduction-6"},"Introduction"),(0,l.kt)("p",null,"Inference PyTorch/ONNX models on NVIDIA GPUs with ",(0,l.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/tensorrt"},"TensorRT"),"."),(0,l.kt)("p",null,"Currently the following models are supported:"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19\nlstm, bert-base, bert-large, gpt2-small")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Do not support large models like ",(0,l.kt)("inlineCode",{parentName:"p"},"gpt2-large")," currently because models larger than 2GB (maximum protobuf size) cannot be exported in one ONNX file.")),(0,l.kt)("h4",{id:"metrics-6"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_mean"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean GPU latency to execute the kernels for a query.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_gpu_time_99"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The 99th percentile GPU latency to execute the kernels for a query.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_mean"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_host_time_99"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The 99th percentile H2D, GPU, and D2H latency to execute the kernels for a query.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_mean"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean duration from when the H2D of a query is called to when the D2H of the same query is completed.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tensorrt-inference/${model}_end_to_end_time_99"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The P99 duration from when the H2D of a query is called to when the D2H of the same query is completed.")))),(0,l.kt)("h3",{id:"ort-inference"},(0,l.kt)("inlineCode",{parentName:"h3"},"ort-inference")),(0,l.kt)("h4",{id:"introduction-7"},"Introduction"),(0,l.kt)("p",null,"Inference performance of the torchvision models using ONNXRuntime. Currently the following models are supported:"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,\nmnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,\nresnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,\nsqueezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19")),(0,l.kt)("p",null,"The supported percentiles are 50, 90, 95, 99, and 99.9."),(0,l.kt)("h4",{id:"metrics-7"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ort-inference/{precision}_{model}_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The mean latency to execute one batch of inference.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ort-inference/{precision}",(0,l.kt)("em",{parentName:"td"},"{model}_time"),"{percentile}"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"The {percentile}th percentile latency to execute one batch of inference.")))),(0,l.kt)("h3",{id:"gpu-burn"},(0,l.kt)("inlineCode",{parentName:"h3"},"gpu-burn")),(0,l.kt)("h4",{id:"introduction-8"},"Introduction"),(0,l.kt)("p",null,"Multi-GPU CUDA stress test for GPU compute and memory utilization, performed by ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/wilicc/gpu-burn"},"gpu-burn"),".\nSupports the use of double unit types and the use of tensor cores."),(0,l.kt)("h4",{id:"metrics-8"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu-burn/time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The runtime for gpu-burn test.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu-burn/gpu_","[0-9]","_pass"),(0,l.kt)("td",{parentName:"tr",align:null},"yes/no"),(0,l.kt)("td",{parentName:"tr",align:null},"The result of the gpu-burn test for each GPU (1: yes, 0: no).")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu-burn/abort"),(0,l.kt)("td",{parentName:"tr",align:null},"yes/no"),(0,l.kt)("td",{parentName:"tr",align:null},"Whether or not GPU-burn test aborted before returning GPU results (1: yes, 0: no).")))),(0,l.kt)("h3",{id:"cpu-hpl"},(0,l.kt)("inlineCode",{parentName:"h3"},"cpu-hpl")),(0,l.kt)("h4",{id:"introduction-9"},"Introduction"),(0,l.kt)("p",null,"HPL or High Performance Computing Linpack evaluates compute bandwidth by solving dense linear systems in double precision arethmetic.\nPerformed by ",(0,l.kt)("a",{parentName:"p",href:"https://netlib.org/benchmark/hpl/"},"High-Performance Linpack Benchmark for Distributed-Memory Computers")),(0,l.kt)("h4",{id:"metrics-9"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-hpl/tests_pass"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"HPL completed running and correctness test has passed (1: pass, 0: fail).")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-hpl/throughput"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GFlops)"),(0,l.kt)("td",{parentName:"tr",align:null},"Compute bandwidth.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-hpl/time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time elapsed during HPL run.")))),(0,l.kt)("h3",{id:"cpu-stream"},(0,l.kt)("inlineCode",{parentName:"h3"},"cpu-stream")),(0,l.kt)("h4",{id:"introduction-10"},"Introduction"),(0,l.kt)("p",null,"Measure of memory bandwidth and computation rate for simple vector kernels.\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://www.cs.virginia.edu/stream/ref.html"},"University of Virginia STREAM benchmark"),"."),(0,l.kt)("h4",{id:"metrics-10"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-stream/threads"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Number of threads used for the test. Determined by core count.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","throughput"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Memory throughput of designated kerel operation.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_avg"),(0,l.kt)("td",{parentName:"tr",align:null},"time (s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Average elapsed times over all iterations.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_min"),(0,l.kt)("td",{parentName:"tr",align:null},"time (s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Minimum elapsed times over all iterations.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-stream/","['copy', 'scale', 'add', 'triad']","_","time_max"),(0,l.kt)("td",{parentName:"tr",align:null},"time (s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Maximum elapsed times over all iterations.")))),(0,l.kt)("h2",{id:"communication-benchmarks"},"Communication Benchmarks"),(0,l.kt)("h3",{id:"cpu-memory-bw-latency"},(0,l.kt)("inlineCode",{parentName:"h3"},"cpu-memory-bw-latency")),(0,l.kt)("h4",{id:"introduction-11"},"Introduction"),(0,l.kt)("p",null,"Measure the memory copy bandwidth and latency across different CPU NUMA nodes.\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html"},"Intel MLC Tool"),"."),(0,l.kt)("h4",{id:"metrics-11"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","bandwidth","_","matrix","_","numa","_","[0-9]","+","_","[0-9]","+","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Former NUMA to latter NUMA memory bandwidth.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","bandwidth","_","matrix","_","numa","_","[0-9]","+","_","[0-9]","+","_","lat"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Former NUMA to latter NUMA memory latency.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","all","_","reads","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, full read.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","3_1","_","reads-writes","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 3 : 1.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","2_1","_","reads-writes","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 2 : 1.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","1_1","_","reads-writes","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, read : write = 1 : 1.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu-memory-bw-latency/mem","_","max","_","bandwidth","_","stream-triad","_","like","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Whole-CPU maximum memory bandwidth, with stream-triad like pattern.")))),(0,l.kt)("h3",{id:"mem-bw"},(0,l.kt)("inlineCode",{parentName:"h3"},"mem-bw")),(0,l.kt)("h4",{id:"introduction-12"},"Introduction"),(0,l.kt)("p",null,"Measure the memory copy bandwidth across PCI-e and memory copy bandwidth between GPUs,\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest"},"NVIDIA"),"\nor ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ROCm-Developer-Tools/HIP/tree/master/samples/1_Utils/hipBusBandwidth"},"AMD")," bandwidth test tool."),(0,l.kt)("h4",{id:"metrics-12"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"mem-bw/h2d_bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Host to device copy bandwidth.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"mem-bw/d2h_bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Device to host copy bandwidth.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"mem-bw/d2d_bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Device to device copy bandwidth.")))),(0,l.kt)("h3",{id:"gpu-copy-bw"},(0,l.kt)("inlineCode",{parentName:"h3"},"gpu-copy-bw")),(0,l.kt)("p",null,"Measure the memory copy bandwidth performed by GPU SM/DMA engine, including device-to-host, host-to-device and device-to-device."),(0,l.kt)("h4",{id:"metrics-13"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu","_","to","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","cpu","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU writing one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading or writing self's memory using DMA engine or GPU SM.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","[0-9]","+","_","(read","|","write)","_","by","_","(sm","|","dma)","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU reading or writing peer GPU's memory using DMA engine or GPU SM with peer communication enabled.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"cpu","_","and","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing one NUMA node's host memory using DMA engine or GPU SM.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","cpu","_","by","_","(sm","|","dma)","_","under","_","numa","[0-9]","+","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"Same as above, but generated by --dtoh --bidirectional.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","gpu","[0-9]","+","_","by","_","(sm","|","dma)","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing self's memory using DMA engine or GPU SM.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","and","_","gpu","[0-9]","+","_","(read","|","write)","_","by","_","(sm","|","dma)","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The bidirectional bandwidth of one GPU reading and writing peer GPU's memory using DMA engine or GPU SM with peer communication enabled.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","[0-9]","+","_","to","_","gpu","_","all","_","write","_","by","_","sm","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of one GPU writing all peer GPUs' memory using GPU SM with peer communication enabled.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","_","all","_","to","_","gpu","[0-9]","+","_","write","_","by","_","sm","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of all peer GPUs writing one GPU's memory using GPU SM with peer communication enabled.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpu","_","all","_","to","_","gpu","_","all","_","write","_","by","_","sm","_","bw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The unidirectional bandwidth of all peer GPUs writing all peer GPUs' memory using GPU SM with peer communication enabled.")))),(0,l.kt)("h3",{id:"ib-loopback"},(0,l.kt)("inlineCode",{parentName:"h3"},"ib-loopback")),(0,l.kt)("h4",{id:"introduction-13"},"Introduction"),(0,l.kt)("p",null,"Measure the InfiniBand loopback verbs bandwidth, performed by\n",(0,l.kt)("a",{parentName:"p",href:"https://github.com/linux-rdma/perftest/tree/7504ce48ac396a02f4d00de359257b2cb8458f06"},"OFED performance tests"),"."),(0,l.kt)("h4",{id:"metrics-14"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.kt)("em",{parentName:"td"},"write_bw"),"${msg_size}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback write bandwidth with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.kt)("em",{parentName:"td"},"read_bw"),"${msg_size}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback read bandwidth with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ib-loopback/ib",(0,l.kt)("em",{parentName:"td"},"send_bw"),"${msg_size}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"InfiniBand loopback send bandwidth with given message size.")))),(0,l.kt)("h3",{id:"nccl-bw--rccl-bw"},(0,l.kt)("inlineCode",{parentName:"h3"},"nccl-bw")," / ",(0,l.kt)("inlineCode",{parentName:"h3"},"rccl-bw")),(0,l.kt)("h4",{id:"introduction-14"},"Introduction"),(0,l.kt)("p",null,"Measure the performance of NCCL/RCCL operations under multi nodes' traffic pattern,\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/nccl-tests/tree/44df0bf010dcc95e840ca0fb7466c67cff3f1f0f"},"nccl-tests"),"\nor ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/ROCmSoftwarePlatform/rccl-tests/tree/dc1ad4853d7ec738387d42a75a58a98d7af00c7b"},"rccl-tests"),".\nSupport the following operations currently: allreduce, allgather, broadcast, reduce, reducescatter, alltoall."),(0,l.kt)("p",null,"Support the following traffic patterns:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"all-nodes"),", validate the NCCL/RCCL performance across all VM nodes simultaneously."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"pair-wise"),", validate the NCCL/RCCL performance across VM pairs with all possible combinations in parallel."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"k-batch"),", validate the NCCL/RCCL performance across VM groups with a specified batch scale."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"topo-aware"),", validate the NCCL/RCCL performance across VM pairs with different distances/hops as a quick test.")),(0,l.kt)("h4",{id:"metrics-15"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"NCCL operation lantency with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_algbw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"NCCL operation algorithm bandwidth with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"nccl-bw/${operation}_${msg_size}_busbw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"NCCL operation bus bandwidth with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"RCCL operation lantency with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_algbw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"RCCL operation algorithm bandwidth with given message size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"rccl-bw/${operation}_${msg_size}_busbw"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"RCCL operation bus bandwidth with given message size.")))),(0,l.kt)("p",null,"If mpi mode is enable and traffic pattern is specified, the metrics pattern will change to ",(0,l.kt)("inlineCode",{parentName:"p"},"nccl-bw/${operation}_${serial_index)_${parallel_index):${msg_size}_time")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"serial_index")," represents the serial index of the host group in serial."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"parallel_index")," represents the parallel index of the host list in parallel.")),(0,l.kt)("h3",{id:"tcp-connectivity"},(0,l.kt)("inlineCode",{parentName:"h3"},"tcp-connectivity")),(0,l.kt)("h4",{id:"introduction-15"},"Introduction"),(0,l.kt)("p",null,"Test the TCP connectivity between current node and nodes in the hostfile,\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/zhengxiaowai/tcping"},"tcping")),(0,l.kt)("h4",{id:"metrics-16"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_successed_count"),(0,l.kt)("td",{parentName:"tr",align:null},"count"),(0,l.kt)("td",{parentName:"tr",align:null},"successed times of tcp connections between current node and other nodes")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_failed_count"),(0,l.kt)("td",{parentName:"tr",align:null},"count"),(0,l.kt)("td",{parentName:"tr",align:null},"failed times of tcp connections between current node and other nodes")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_success_rate"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"success rate (successed/total) of tcp connection between current node and other nodes")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_min"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"mininum latency of tcp connections between current node and other nodes")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_max"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"maximum latency of tcp connections between current node and other nodes")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"tcp-connectivity/${hostname/ip}_time_avg"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"average latency of tcp connections between current node and other nodes")))),(0,l.kt)("h3",{id:"gpcnet-network-test--gpcnet-network-load-test"},(0,l.kt)("inlineCode",{parentName:"h3"},"gpcnet-network-test")," / ",(0,l.kt)("inlineCode",{parentName:"h3"},"gpcnet-network-load-test")),(0,l.kt)("h4",{id:"introduction-16"},"Introduction"),(0,l.kt)("p",null,"Distributed test, test the global network performance and congestion,\nperformed by ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/netbench/GPCNET"},"GPCNET")),(0,l.kt)("p",null,"gpcnet-network-test: Full system network tests in random and natural ring, alltoall and allreduce, at least 2 nodes"),(0,l.kt)("p",null,"gpcnet-network-load-test: Select full system network tests run with four congestors to measure network congestion or contention, at least 10 nodes"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"supporting network tests: RR Two-sided Lat (8 B), RR Get Lat (8 B), RR Two-sided BW (131072 B), RR Put BW (131072 B), RR Two-sided BW+Sync (131072 B), Nat Two-sided BW (131072 B), Multiple Allreduce (8 B), Multiple Alltoall (4096 B)"),(0,l.kt)("li",{parentName:"ul"},"supporting congestors: Alltoall (4096 B), Two-sided Incast (4096 B), Put Incast (4096 B), Get Bcast (4096 B)")),(0,l.kt)("h4",{id:"metrics-17"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.kt)("em",{parentName:"td"},"two-sided_lat"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side latency' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.kt)("em",{parentName:"td"},"two-sided+sync_bw"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.kt)("td",{parentName:"tr",align:null},"fstatistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'random ring communication pattern two-side bandwidth with barrier' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,l.kt)("em",{parentName:"td"},"allreduce_time"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'multiple allreduce bandwidth' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.kt)("em",{parentName:"td"},"get_lat"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR GetLat (8 B)' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/rr",(0,l.kt)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'RR Two-sidedBW (131072 B)' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/nat",(0,l.kt)("em",{parentName:"td"},"two-sided_bw"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Nat Two-sidedBW (131072 B)' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-test/multiple",(0,l.kt)("em",{parentName:"td"},"alltoall_bw"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (MiB/s/rank)"),(0,l.kt)("td",{parentName:"tr",align:null},"statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm 'Multiple Alltoall (4096 B)' for network testing")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,l.kt)("em",{parentName:"td"},"two-sided_lat_x"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/rr",(0,l.kt)("em",{parentName:"td"},"two-sided+sync_bw_x"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"gpcnet-network-load-test/multiple",(0,l.kt)("em",{parentName:"td"},"allreduce_x"),"${stat}"),(0,l.kt)("td",{parentName:"tr",align:null},"factor (x)"),(0,l.kt)("td",{parentName:"tr",align:null},"summary about congestion impact factor of the network test algorithm")))),(0,l.kt)("h3",{id:"ib-traffic"},(0,l.kt)("inlineCode",{parentName:"h3"},"ib-traffic")),(0,l.kt)("h4",{id:"introduction-17"},"Introduction"),(0,l.kt)("p",null,"Measure the InfiniBand performance under multi nodes' traffic pattern."),(0,l.kt)("p",null,"The direction between client and server can be 'cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu'."),(0,l.kt)("p",null,"The traffic pattern is defined in a config file, which is pre-defined for one-to-many, many-to-one and all-to-all patterns.\nEach row in the config is one round, and all pairs of nodes in a row run ib command simultaneously."),(0,l.kt)("p",null,"Besides the above three patterns, ib-traffic also supports topology-aware traffic pattern. To run ib-traffic with topology-aware\npattern, the user needs to specify 3 required (and 2 optional) parameters in YAML config file:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"--pattern\t","\u2003",(0,l.kt)("strong",{parentName:"li"},"topo-aware")),(0,l.kt)("li",{parentName:"ul"},"--ibstat\t","\u2003",(0,l.kt)("strong",{parentName:"li"},"path to ibstat output")),(0,l.kt)("li",{parentName:"ul"},"--ibnetdiscover\t","\u2003",(0,l.kt)("strong",{parentName:"li"},"path to ibnetdiscover output")),(0,l.kt)("li",{parentName:"ul"},"--min_dist\t","\u2003",(0,l.kt)("strong",{parentName:"li"},"minimum distance of VM pairs (optional, default 2)")),(0,l.kt)("li",{parentName:"ul"},"--max_dist\t","\u2003",(0,l.kt)("strong",{parentName:"li"},"maximum distance of VM pairs (optional, default 6)"))),(0,l.kt)("p",null,"Each row in the config file has all VM pairs with a fixed distance (#hops). That's by default, 1st, 2nd, 3rd row has all VM pairs\nwith topology distance of 2, 4, 6, respectively."),(0,l.kt)("h4",{id:"metrics-18"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Metrics"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ib-traffic/ib","_","write","_","bw","_","${msg_size}","_","${direction}","_","${line}","_","${pair}:${server}","_","${client}"),(0,l.kt)("td",{parentName:"tr",align:null},"bandwidth (GB/s)"),(0,l.kt)("td",{parentName:"tr",align:null},"The max bandwidth of perftest (ib_write_bw, ib_send_bw, ib_read_bw) using ${msg_size} with ${direction}('cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu') run between the ${pair}",(0,l.kt)("sup",null,"th")," node pair in the ${line}",(0,l.kt)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"ib-traffic/ib","_","write","_","lat","_","${msg_size}","_","${direction}","_","${line}","_","${pair}:${server}","_","${client}"),(0,l.kt)("td",{parentName:"tr",align:null},"time (us)"),(0,l.kt)("td",{parentName:"tr",align:null},"The max latency of perftest (ib_write_lat, ib_send_lat, ib_read_lat) using ${msg_size} with ${direction}('cpu-to-cpu'/'gpu-to-gpu'/'gpu-to-cpu'/'cpu-to-gpu') run between the ${pair}",(0,l.kt)("sup",null,"th")," node pair in the ${line}",(0,l.kt)("sup",null,"th")," line of the config, ${server} and ${client} are the hostname of server and client.")))),(0,l.kt)("h2",{id:"computation-communication-benchmarks"},"Computation-communication Benchmarks"),(0,l.kt)("h3",{id:"computation-communication-overlap"},(0,l.kt)("inlineCode",{parentName:"h3"},"computation-communication-overlap")),(0,l.kt)("h4",{id:"introduction-18"},"Introduction"),(0,l.kt)("p",null,"Test the performance of single node when communication and computation overlap."),(0,l.kt)("h4",{id:"metrics-19"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/mul_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time of communication and mul kernel computation overlap.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-computation-communication-overlap/matmul_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time of communication and matmul kernel computation overlap.")))),(0,l.kt)("h4",{id:""}),(0,l.kt)("h3",{id:"sharding-matmul"},(0,l.kt)("inlineCode",{parentName:"h3"},"sharding-matmul")),(0,l.kt)("h4",{id:"introduction-19"},"Introduction"),(0,l.kt)("p",null,"Test the performance of large scale matmul operation with multiple GPUs:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"allreduce: Each GPU will calculate part of the MM calculation, and use AllReduce to merge all data into one tensor."),(0,l.kt)("li",{parentName:"ul"},"allgather: Each GPU will calculate part of the MM calculation, and use AllGather + Concat to merge all data into one tensor.")),(0,l.kt)("h4",{id:"metrics-20"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allreduce_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time of sharding matmul using allreduce.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-sharding-matmul/allgather_time"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Time of sharding matmul using allgather.")))),(0,l.kt)("h3",{id:"dist-inference"},(0,l.kt)("inlineCode",{parentName:"h3"},"dist-inference")),(0,l.kt)("h4",{id:"introduction-20"},"Introduction"),(0,l.kt)("p",null,"Test the performance of distributed model inference. Support both PyTorch implementation and cpp implementation."),(0,l.kt)("h4",{id:"metrics-21"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-dist-inference/step_times"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Average time of model inference runs.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"pytorch-dist-inference/step",(0,l.kt)("em",{parentName:"td"},"times"),"${percentile}"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ms)"),(0,l.kt)("td",{parentName:"tr",align:null},"Tail (50,90,95,99,99.9) time of model inference runs.")))),(0,l.kt)("h2",{id:"storage-benchmarks"},"Storage Benchmarks"),(0,l.kt)("h3",{id:"disk-benchmark"},(0,l.kt)("inlineCode",{parentName:"h3"},"disk-benchmark")),(0,l.kt)("h4",{id:"introduction-21"},"Introduction"),(0,l.kt)("p",null,"Measure the disk performance through ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/axboe/fio/tree/0313e938c9c8bb37d71dade239f1f5326677b079"},"FIO"),"."),(0,l.kt)("h4",{id:"metrics-22"},"Metrics"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Name"),(0,l.kt)("th",{parentName:"tr",align:null},"Unit"),(0,l.kt)("th",{parentName:"tr",align:null},"Description"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_bs"),(0,l.kt)("td",{parentName:"tr",align:null},"size (bytes)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write block size.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_iops"),(0,l.kt)("td",{parentName:"tr",align:null},"IOPS"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write read IOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_95.0"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 95.0 percentile.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.0"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.0 percentile.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.9"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write read latency in 99.9 percentile.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_iops"),(0,l.kt)("td",{parentName:"tr",align:null},"IOPS"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write write IOPS.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_95.0"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 95.0 percentile.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.0"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.0 percentile.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.9"),(0,l.kt)("td",{parentName:"tr",align:null},"time (ns)"),(0,l.kt)("td",{parentName:"tr",align:null},"Disk random read write write latency in 99.9 percentile.")))))}c.isMDXComponent=!0}}]);
---
id: data-diagnosis
---

# Data Diagnosis

## Introduction

Run baseline-based data diagnosis for the results which are output from superbench runner of multiple nodes.

## Input

The input mainly includes 2 files:

 - **raw data**: jsonl file including multi nodes' results automatically generated by superbench runner, the location of file should be in ${output-dir}/results-summary.jsonl.
 - **baseline**: yaml format file including the rules and criteria for benchmarks and metrics used for diagnosis.

## Baseline file

This section covers schema of SuperBench baseline YAML file.

The Conventions are the same with [Superbench Config File](https://microsoft.github.io/superbenchmark/docs/superbench-config), please view it first.

This baseline file describes the criteria and rules of the metrics used for data diagnosis. They are firstly organized by the benchmark name, and each benchmark includes several metrics or regex. One metric or regex should have a criteria and a rule, which indicates that these metrics will use this rule to do the diagnosis.

Here is an overview of baseline fule structure:

scheme:
```yaml
version: string
superbench:
  enable: string | [ string ]
  var:
    ${var_name}: dict
  benchmarks:
    ${benchmark_name}: 
      enable: bool
      metrics:
        ${metric|regex}:
          criteria: number
          rules: dict
        ${metric|regex}:
          criteria: number
          rules: dict
        ...
```

example:
```yaml
# SuperBench baseline
version: v0.3
superbench:
  enable: null
  var:
    default_value_rule: &default_value_rule
      rules:
        name: value
    return_code_rule: &return_code_rule
      .*/return_code:
        criteria: 0
        <<: *default_value_rule
  benchmarks:
    kernel-launch:
        enable: true
        metrics:
          <<: *return_code_rule
          kernel-launch/event_overhead:\d+: 
              criteria: 0.1
              rules:
                name: variance
                condition: 0.05    
```

### `metrics`

Metrics can be defined either metric full name or regex for convenience.
Each benchmark must contain a default metric of return_code as the above `&return_code_rule` in the example, which is used to identify miss test. 

### `criteria`

The criteria number for the metric, it must be a numerical value.

### `rules`

The rule used for the metric to compare the raw data and criteria.

2 rules are supported currently:

`variance`:

 - `name`: variance
 - `condition`: the variance value between raw data and criteria
  
If the condition is positive, the rule is that the variance should be lower than condition; if the condition is negative, the rule is that the variance should be larger than condition.


`value`:

 - `name`: value

The rule is that the raw data should be smaller than the criteria.

## Output

The ouput is a excel format file including the raw data in 'Raw Data' sheet and the issued nodes with details and processed data according rules in 'Not Accept' sheet.

- index: issued node name

- Hw Issues: whether the issued node is caused by hardware issue. If the issues not include model benchmark, TensorRT and ORT, this will be true

- Category: benchmarks in which there is any metric violating the rules

- Issue Details: the metrics violate the rules

- ${metric}: processed data of the ${metric}, if the rule is `variance`, the value should be variance between raw data and criteria; if the rule is `value`, the value is raw data.

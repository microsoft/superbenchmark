---
id: data-diagnosis
---

# Data Diagnosis

## Introduction

This tool is to validate and diagnose the SuperBench benchmarking results for hundreds or thousands of servers automatically as to whether they meet the hardware performance/quality standard according to the baseline of metrics in the benchmarks,  and finally provides a list of failed servers with detailed information.

## Input

The input mainly includes 2 files:

 - **raw data**: jsonl file including multi nodes' results automatically generated by SuperBench runner.

    `Tips: the file can be found as ${output-dir}/results-summary.jsonl after each successfully run.`

 - **baseline**: yaml format file including the rules and criteria for the metrics in the benchmarks used for diagnosis.

### Baseline file

This section covers the writing schema of SuperBench baseline YAML file.

The Conventions are the same with [SuperBench Config File](https://microsoft.github.io/superbenchmark/docs/superbench-config), please view it first.

This baseline file describes the criteria and rules of the metrics used for data diagnosis. They are firstly organized by the benchmark name, and each benchmark includes several metrics or regex. One metric or regex should have criteria and a rule, which indicates that these metrics will use this rule to do the diagnosis.

Here is an overview of baseline fule structure:

scheme:
```yaml
version: string
superbench:
  enable: string | [ string ]
  var:
    ${var_name}: dict
  benchmarks:
    ${benchmark_name}:
      enable: bool
      metrics:
        ${metric|regex}:
          criteria: number
          rules: dict
        ${metric|regex}:
          criteria: number
          rules: dict
        ...
```

example:
```yaml
# SuperBench baseline
version: v0.3
superbench:
  enable: null
  var:
    default_variance_rule: &default_variance_rule
      rules:
        name: variance
        condition: -0.05
    default_value_rule: &default_value_rule
      rules:
        name: value
    return_code_rule: &return_code_rule
      .*/return_code:
        criteria: 0
        <<: *default_value_rule
  benchmarks:
    kernel-launch:
        enable: true
        metrics:
          kernel-launch/event_overhead:\d+:
              criteria: 0.1
              rules:
                name: variance
                condition: 0.05
          kernel-launch/wall_overhead:\d+:
              criteria: 0.1
              rules:
                name: variance
                condition: 0.05
          <<: *return_code_rule
    mem-bw:
        enable: true
        metrics:
          mem-bw/H2D_Mem_BW:\d+:
              criteria: 25.6
              <<: *default_variance_rule
          <<: *return_code_rule
```

#### `metrics`

Metrics can be defined as either metric full name or regex for convenience.
Each benchmark used for diagnosis must contain a default metric of return_code as the above `&return_code_rule` in the example, which is used to identify miss test.

#### `criteria`

The criteria number for the metric, it must be a numerical value.

#### `rules`

The rule is used to determine if the raw data meets the criteria for each metric.

2 types of rules are supported currently:

 `variance`:

   - `name`: variance
   - `condition`: the variance value between raw data and criteria

  If the condition is positive, the rule is that the variance should be lower than the condition; if the condition is negative, the rule is that the variance should be larger than the condition.


  `value`:

  - `name`: value

  The rule is that the raw data should be smaller than the criteria.

## Output

The output is an excel format file including the raw data in 'Raw Data' sheet and the issued nodes with details and processed data according to rules in 'Not Accept' sheet.

- index: issued node name

- Hw Issues: whether the issued node is caused by hardware issues. If the issues do not include model benchmark, TensorRT and ORT, this will be true

- Category: benchmarks in which there is any metric violating the rules

- Issue Details: the metrics violate the rules

- ${metric}: processed data of the ${metric}, if the rule is `variance`, the value should be variance between raw data and criteria; if the rule is `value`, the value is raw data.

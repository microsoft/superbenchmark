<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.1">
<link rel="alternate" type="application/rss+xml" href="/superbenchmark/blog/rss.xml" title="SuperBench Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/superbenchmark/blog/atom.xml" title="SuperBench Blog Atom Feed">
<link rel="search" type="application/opensearchdescription+xml" title="SuperBench" href="/superbenchmark/opensearch.xml"><title data-react-helmet="true">Micro Benchmarks | SuperBench</title><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Micro Benchmarks | SuperBench"><meta data-react-helmet="true" name="description" content="Computation Benchmarks"><meta data-react-helmet="true" property="og:description" content="Computation Benchmarks"><link data-react-helmet="true" rel="shortcut icon" href="/superbenchmark/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/superbenchmark/assets/css/styles.081fa1d1.css">
<link rel="preload" href="/superbenchmark/assets/js/runtime~main.1636966a.js" as="script">
<link rel="preload" href="/superbenchmark/assets/js/main.fa6a49aa.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP shadow--md">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/superbenchmark/"><img src="/superbenchmark/img/logo.svg" alt="Docusaurus Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/superbenchmark/img/logo.svg" alt="Docusaurus Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">SuperBench</b></a><a class="navbar__item navbar__link navbar__link--active" href="/superbenchmark/docs/introduction">Docs</a><a class="navbar__item navbar__link" href="/superbenchmark/docs/cli">API</a><a class="navbar__item navbar__link" href="/superbenchmark/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/superbenchmark" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="react-toggle displayOnlyInLargeViewport_GrZ2 react-toggle--disabled"><div class="react-toggle-track" role="button" tabindex="-1"><div class="react-toggle-track-check"><span class="toggle_71bT">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">ðŸŒž</span></div><div class="react-toggle-thumb"></div></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div><div class="searchBox_xXbB"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/superbenchmark/"><img src="/superbenchmark/img/logo.svg" alt="Docusaurus Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/superbenchmark/img/logo.svg" alt="Docusaurus Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">SuperBench</b></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link navbar__link--active" href="/superbenchmark/docs/introduction">Docs</a></li><li class="menu__list-item"><a class="menu__link" href="/superbenchmark/docs/cli">API</a></li><li class="menu__list-item"><a class="menu__link" href="/superbenchmark/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/microsoft/superbenchmark" target="_blank" rel="noopener noreferrer" class="menu__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_31aa"><aside class="docSidebarContainer_3Kbt"><div class="sidebar_15mo"><nav class="menu menu--responsive thin-scrollbar menu_Bmed" aria-label="Sidebar navigation"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/superbenchmark/docs/introduction">Introduction</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!">Getting Started</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/getting-started/installation">Installation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/getting-started/configuration">Configuration</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/getting-started/run-superbench">Run SuperBench</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">User Tutorial</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!" tabindex="0">Benchmarks</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/superbenchmark/docs/user-tutorial/benchmarks/micro-benchmarks">Micro Benchmarks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks">Model Benchmarks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/benchmarks/docker-benchmarks">Docker Benchmarks</a></li></ul></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/system-config">System Config Info</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/data-diagnosis">Data Diagnosis</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/monitor">Monitor</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/superbenchmark/docs/user-tutorial/container-images">Container Images</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Developer Guides</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/superbenchmark/docs/developer-guides/development">Development</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/superbenchmark/docs/developer-guides/using-docker">Using Docker</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/superbenchmark/docs/developer-guides/contributing">Contributing</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Design Docs</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/superbenchmark/docs/design-docs/overview">Superbench Design</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/superbenchmark/docs/design-docs/benchmarks">Benchmarks Design</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_3ufF"><div class="container padding-top--md padding-bottom--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="markdown"><header><h1 class="h1Heading_27L5">Micro Benchmarks</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="computation-benchmarks"></a>Computation Benchmarks<a class="hash-link" href="#computation-benchmarks" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="kernel-launch"></a><code>kernel-launch</code><a class="hash-link" href="#kernel-launch" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h4><p>Measure GPU kernel launch latency,
which is defined as the time range from the beginning of the launch API call to the beginning of the kernel execution.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics"></a>Metrics<a class="hash-link" href="#metrics" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>kernel-launch/event_time</td><td>time (ms)</td><td>Launch latency measured in GPU time.</td></tr><tr><td>kernel-launch/wall_time</td><td>time (ms)</td><td>Launch latency measured in CPU time.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gemm-flops"></a><code>gemm-flops</code><a class="hash-link" href="#gemm-flops" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-1"></a>Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">#</a></h4><p>Measure the GPU GEMM FLOPS for different float and int data types, with or without Tensor Core (XDLOPS),
performed by NVIDIA <a href="https://github.com/NVIDIA/cutlass/tree/ccb697bac77fcc898e9c897b2c90aa5b60ac72fb" target="_blank" rel="noopener noreferrer">cutlass</a>
or AMD <a href="https://github.com/ROCmSoftwarePlatform/rocBLAS/tree/develop/clients/benchmarks" target="_blank" rel="noopener noreferrer">rocblas-bench</a>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-1"></a>Metrics<a class="hash-link" href="#metrics-1" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gemm-flops/fp64_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float64 peak FLOPS.</td></tr><tr><td>gemm-flops/fp32_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float32 peak FLOPS.</td></tr><tr><td>gemm-flops/fp16_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS.</td></tr><tr><td>gemm-flops/fp64_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float64 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/tf32_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM tensor-float32 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/fp16_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/bf16_tc_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM bfloat16 peak FLOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/int8_tc_iops</td><td>IOPS (GIOPS)</td><td>GEMM int8 peak IOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/int4_tc_iops</td><td>IOPS (GIOPS)</td><td>GEMM int4 peak IOPS with NVIDIA Tensor Core.</td></tr><tr><td>gemm-flops/fp32_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM tensor-float32 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/fp16_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM float16 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/bf16_xdlops_flops</td><td>FLOPS (GFLOPS)</td><td>GEMM bfloat16 peak FLOPS with AMD XDLOPS.</td></tr><tr><td>gemm-flops/int8_xdlops_iops</td><td>IOPS (GIOPS)</td><td>GEMM int8 peak IOPS with AMD XDLOPS.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="matmul"></a><code>matmul</code><a class="hash-link" href="#matmul" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-2"></a>Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">#</a></h4><p>Large scale matmul operation using <code>torch.matmul</code> with one GPU.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-2"></a>Metrics<a class="hash-link" href="#metrics-2" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-matmul/nosharding_time</td><td>time (ms)</td><td>Time of pure matmul operation.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="cublas-function"></a><code>cublas-function</code><a class="hash-link" href="#cublas-function" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-3"></a>Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">#</a></h4><p>Measure the performance of most common Nvidia cuBLAS functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2.</p><p>The supported functions for cuBLAS are as follows:</p><ul><li>cublasSgemm</li><li>cublasSgemmStridedBatched</li><li>cublasGemmStridedBatchedEx</li><li>cublasGemmEx</li><li>cublasCgemm3mStridedBatched</li><li>cublasCgemm</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-3"></a>Metrics<a class="hash-link" href="#metrics-3" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cublas-function/name<em>${function_name}</em>${parameters}_time</td><td>time (us)</td><td>The mean time to execute the cublas function with the parameters.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="cudnn-function"></a><code>cudnn-function</code><a class="hash-link" href="#cudnn-function" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-4"></a>Introduction<a class="hash-link" href="#introduction-4" title="Direct link to heading">#</a></h4><p>Measure the performance of most common Nvidia cuDNN functions with parameters in models training including ResNet, VGG, DenseNet, LSTM, BERT, and GPT-2.</p><p>The supported functions for cuDNN are as follows:</p><ul><li>cudnnConvolutionBackwardFilter</li><li>cudnnConvolutionBackwardData</li><li>cudnnConvolutionForward</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-4"></a>Metrics<a class="hash-link" href="#metrics-4" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cudnn-function/name<em>${function_name}</em>${parameters}_time</td><td>time (us)</td><td>The mean time to execute the cudnn function with the parameters.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tensorrt-inference"></a><code>tensorrt-inference</code><a class="hash-link" href="#tensorrt-inference" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-5"></a>Introduction<a class="hash-link" href="#introduction-5" title="Direct link to heading">#</a></h4><p>Inference PyTorch/ONNX models on NVIDIA GPUs with <a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener noreferrer">TensorRT</a>.</p><p>Currently the following models are supported:</p><blockquote><p>alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,
mnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,
resnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,
squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19
lstm, bert-base, bert-large, gpt2-small</p></blockquote><blockquote><p>Do not support large models like <code>gpt2-large</code> currently because models larger than 2GB (maximum protobuf size) cannot be exported in one ONNX file.</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-5"></a>Metrics<a class="hash-link" href="#metrics-5" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>tensorrt-inference/${model}_gpu_time_mean</td><td>time (ms)</td><td>The mean GPU latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_gpu_time_99</td><td>time (ms)</td><td>The 99th percentile GPU latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_host_time_mean</td><td>time (ms)</td><td>The mean H2D, GPU, and D2H latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_host_time_99</td><td>time (ms)</td><td>The 99th percentile H2D, GPU, and D2H latency to execute the kernels for a query.</td></tr><tr><td>tensorrt-inference/${model}_end_to_end_time_mean</td><td>time (ms)</td><td>The mean duration from when the H2D of a query is called to when the D2H of the same query is completed.</td></tr><tr><td>tensorrt-inference/${model}_end_to_end_time_99</td><td>time (ms)</td><td>The P99 duration from when the H2D of a query is called to when the D2H of the same query is completed.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="ort-inference"></a><code>ort-inference</code><a class="hash-link" href="#ort-inference" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-6"></a>Introduction<a class="hash-link" href="#introduction-6" title="Direct link to heading">#</a></h4><p>Inference performance of the torchvision models using ONNXRuntime. Currently the following models are supported:</p><blockquote><p>alexnet, densenet121, densenet169, densenet201, densenet161, googlenet, inception_v3, mnasnet0_5,
mnasnet1_0, mobilenet_v2, resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d,
resnext101_32x8d, wide_resnet50_2, wide_resnet101_2, shufflenet_v2_x0_5, shufflenet_v2_x1_0,
squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19</p></blockquote><p>The supported percentiles are 50, 90, 95, 99, and 99.9.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-6"></a>Metrics<a class="hash-link" href="#metrics-6" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ort-inference/{precision}_{model}_time</td><td>time (ms)</td><td>The mean latency to execute one batch of inference.</td></tr><tr><td>ort-inference/{precision}<em>{model}_time</em>{percentile}</td><td>time (ms)</td><td>The {percentile}th percentile latency to execute one batch of inference.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gpu-burn"></a><code>gpu-burn</code><a class="hash-link" href="#gpu-burn" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-7"></a>Introduction<a class="hash-link" href="#introduction-7" title="Direct link to heading">#</a></h4><p>Multi-GPU CUDA stress test for GPU compute and memory utilization, performed by <a href="https://github.com/wilicc/gpu-burn" target="_blank" rel="noopener noreferrer">gpu-burn</a>.
Supports the use of double unit types and the use of tensor cores.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-7"></a>Metrics<a class="hash-link" href="#metrics-7" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gpu-burn/time</td><td>time (s)</td><td>The runtime for gpu-burn test.</td></tr><tr><td>gpu-burn/gpu_[0-9]_pass</td><td>yes/no</td><td>The result of the gpu-burn test for each GPU (1: yes, 0: no).</td></tr><tr><td>gpu-burn/abort</td><td>yes/no</td><td>Whether or not GPU-burn test aborted before returning GPU results (1: yes, 0: no).</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="communication-benchmarks"></a>Communication Benchmarks<a class="hash-link" href="#communication-benchmarks" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="cpu-memory-bw-latency"></a><code>cpu-memory-bw-latency</code><a class="hash-link" href="#cpu-memory-bw-latency" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-8"></a>Introduction<a class="hash-link" href="#introduction-8" title="Direct link to heading">#</a></h4><p>Measure the memory copy bandwidth and latency across different CPU NUMA nodes.
performed by <a href="https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html" target="_blank" rel="noopener noreferrer">Intel MLC Tool</a>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-8"></a>Metrics<a class="hash-link" href="#metrics-8" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cpu-memory-bw-latency/mem_bandwidth_matrix_numa_[0-9]+_[0-9]+_bw</td><td>bandwidth (GB/s)</td><td>Former NUMA to latter NUMA memory bandwidth.</td></tr><tr><td>cpu-memory-bw-latency/mem_bandwidth_matrix_numa_[0-9]+_[0-9]+_lat</td><td>time (us)</td><td>Former NUMA to latter NUMA memory latency.</td></tr><tr><td>cpu-memory-bw-latency/mem_max_bandwidth_all_reads_bw</td><td>bandwidth (GB/s)</td><td>Whole-CPU maximum memory bandwidth, full read.</td></tr><tr><td>cpu-memory-bw-latency/mem_max_bandwidth_3_1_reads-writes_bw</td><td>bandwidth (GB/s)</td><td>Whole-CPU maximum memory bandwidth, read : write = 3 : 1.</td></tr><tr><td>cpu-memory-bw-latency/mem_max_bandwidth_2_1_reads-writes_bw</td><td>bandwidth (GB/s)</td><td>Whole-CPU maximum memory bandwidth, read : write = 2 : 1.</td></tr><tr><td>cpu-memory-bw-latency/mem_max_bandwidth_1_1_reads-writes_bw</td><td>bandwidth (GB/s)</td><td>Whole-CPU maximum memory bandwidth, read : write = 1 : 1.</td></tr><tr><td>cpu-memory-bw-latency/mem_max_bandwidth_stream-triad_like_bw</td><td>bandwidth (GB/s)</td><td>Whole-CPU maximum memory bandwidth, with stream-triad like pattern.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="mem-bw"></a><code>mem-bw</code><a class="hash-link" href="#mem-bw" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-9"></a>Introduction<a class="hash-link" href="#introduction-9" title="Direct link to heading">#</a></h4><p>Measure the memory copy bandwidth across PCI-e and memory copy bandwidth between GPUs,
performed by <a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/bandwidthTest" target="_blank" rel="noopener noreferrer">NVIDIA</a>
or <a href="https://github.com/ROCm-Developer-Tools/HIP/tree/master/samples/1_Utils/hipBusBandwidth" target="_blank" rel="noopener noreferrer">AMD</a> bandwidth test tool.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-9"></a>Metrics<a class="hash-link" href="#metrics-9" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>mem-bw/h2d_bw</td><td>bandwidth (GB/s)</td><td>Host to device copy bandwidth.</td></tr><tr><td>mem-bw/d2h_bw</td><td>bandwidth (GB/s)</td><td>Device to host copy bandwidth.</td></tr><tr><td>mem-bw/d2d_bw</td><td>bandwidth (GB/s)</td><td>Device to device copy bandwidth.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gpu-copy-bw"></a><code>gpu-copy-bw</code><a class="hash-link" href="#gpu-copy-bw" title="Direct link to heading">#</a></h3><p>Measure the memory copy bandwidth performed by GPU SM/DMA engine, including device-to-host, host-to-device and device-to-device.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-10"></a>Metrics<a class="hash-link" href="#metrics-10" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>cpu_to_gpu[0-9]+_by_(sm|dma)_under_numa[0-9]+_bw</td><td>bandwidth (GB/s)</td><td>The unidirectional bandwidth of one GPU reading one NUMA node&#x27;s host memory using DMA engine or GPU SM.</td></tr><tr><td>gpu[0-9]+_to_cpu_by_(sm|dma)_under_numa[0-9]+_bw</td><td>bandwidth (GB/s)</td><td>The unidirectional bandwidth of one GPU writing one NUMA node&#x27;s host memory using DMA engine or GPU SM.</td></tr><tr><td>gpu[0-9]+_to_gpu[0-9]+_by_(sm|dma)_bw</td><td>bandwidth (GB/s)</td><td>The unidirectional bandwidth of one GPU reading or writing self&#x27;s memory using DMA engine or GPU SM.</td></tr><tr><td>gpu[0-9]+_to_gpu[0-9]+_(read|write)_by_(sm|dma)_bw</td><td>bandwidth (GB/s)</td><td>The unidirectional bandwidth of one GPU reading or writing peer GPU&#x27;s memory using DMA engine or GPU SM with peer communication enabled.</td></tr><tr><td>cpu_and_gpu[0-9]+_by_(sm|dma)_under_numa[0-9]+_bw</td><td>bandwidth (GB/s)</td><td>The bidirectional bandwidth of one GPU reading and writing one NUMA node&#x27;s host memory using DMA engine or GPU SM.</td></tr><tr><td>gpu[0-9]+_and_cpu_by_(sm|dma)_under_numa[0-9]+_bw</td><td>bandwidth (GB/s)</td><td>Same as above, but generated by --dtoh --bidirectional.</td></tr><tr><td>gpu[0-9]+_and_gpu[0-9]+_by_(sm|dma)_bw</td><td>bandwidth (GB/s)</td><td>The bidirectional bandwidth of one GPU reading and writing self&#x27;s memory using DMA engine or GPU SM.</td></tr><tr><td>gpu[0-9]+_and_gpu[0-9]+_(read|write)_by_(sm|dma)_bw</td><td>bandwidth (GB/s)</td><td>The bidirectional bandwidth of one GPU reading and writing peer GPU&#x27;s memory using DMA engine or GPU SM with peer communication enabled.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="ib-loopback"></a><code>ib-loopback</code><a class="hash-link" href="#ib-loopback" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-10"></a>Introduction<a class="hash-link" href="#introduction-10" title="Direct link to heading">#</a></h4><p>Measure the InfiniBand loopback verbs bandwidth, performed by
<a href="https://github.com/linux-rdma/perftest/tree/7504ce48ac396a02f4d00de359257b2cb8458f06" target="_blank" rel="noopener noreferrer">OFED performance tests</a>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-11"></a>Metrics<a class="hash-link" href="#metrics-11" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ib-loopback/ib<em>write</em>${msg_size}_ib[0-9]_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback write bandwidth with given message size.</td></tr><tr><td>ib-loopback/ib<em>read</em>${msg_size}_ib[0-9]_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback read bandwidth with given message size.</td></tr><tr><td>ib-loopback/ib<em>send</em>${msg_size}_ib[0-9]_bw</td><td>bandwidth (GB/s)</td><td>InfiniBand loopback send bandwidth with given message size.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="nccl-bw--rccl-bw"></a><code>nccl-bw</code> / <code>rccl-bw</code><a class="hash-link" href="#nccl-bw--rccl-bw" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-11"></a>Introduction<a class="hash-link" href="#introduction-11" title="Direct link to heading">#</a></h4><p>Measure the performance of NCCL/RCCL operations,
performed by <a href="https://github.com/NVIDIA/nccl-tests/tree/44df0bf010dcc95e840ca0fb7466c67cff3f1f0f" target="_blank" rel="noopener noreferrer">nccl-tests</a>
or <a href="https://github.com/ROCmSoftwarePlatform/rccl-tests/tree/dc1ad4853d7ec738387d42a75a58a98d7af00c7b" target="_blank" rel="noopener noreferrer">rccl-tests</a>.
Support the following operations currently: allreduce, allgather, broadcast, reduce, reducescatter, alltoall.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-12"></a>Metrics<a class="hash-link" href="#metrics-12" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>nccl-bw/${operation}_${msg_size}_time</td><td>time (us)</td><td>NCCL operation lantency with given message size.</td></tr><tr><td>nccl-bw/${operation}_${msg_size}_algbw</td><td>bandwidth (GB/s)</td><td>NCCL operation algorithm bandwidth with given message size.</td></tr><tr><td>nccl-bw/${operation}_${msg_size}_busbw</td><td>bandwidth (GB/s)</td><td>NCCL operation bus bandwidth with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_time</td><td>time (us)</td><td>RCCL operation lantency with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_algbw</td><td>bandwidth (GB/s)</td><td>RCCL operation algorithm bandwidth with given message size.</td></tr><tr><td>rccl-bw/${operation}_${msg_size}_busbw</td><td>bandwidth (GB/s)</td><td>RCCL operation bus bandwidth with given message size.</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tcp-connectivity"></a><code>tcp-connectivity</code><a class="hash-link" href="#tcp-connectivity" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-12"></a>Introduction<a class="hash-link" href="#introduction-12" title="Direct link to heading">#</a></h4><p>Test the TCP connectivity between current node and nodes in the hostfile,
performed by <a href="https://github.com/zhengxiaowai/tcping" target="_blank" rel="noopener noreferrer">tcping</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-13"></a>Metrics<a class="hash-link" href="#metrics-13" title="Direct link to heading">#</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>tcp-connectivity/${hostname/ip}_successed_count</td><td>count</td><td>successed times of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_failed_count</td><td>count</td><td>failed times of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_success_rate</td><td></td><td>success rate (successed/total) of tcp connection between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_min</td><td>time (ms)</td><td>mininum latency of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_max</td><td>time (ms)</td><td>maximum latency of tcp connections between current node and other nodes</td></tr><tr><td>tcp-connectivity/${hostname/ip}_time_avg</td><td>time (ms)</td><td>average latency of tcp connections between current node and other nodes</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="gpcnet-network-test--gpcnet-network-load-test"></a><code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code><a class="hash-link" href="#gpcnet-network-test--gpcnet-network-load-test" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-13"></a>Introduction<a class="hash-link" href="#introduction-13" title="Direct link to heading">#</a></h4><p>Distributed test, test the global network performance and congestion,
performed by <a href="https://github.com/netbench/GPCNET" target="_blank" rel="noopener noreferrer">GPCNET</a></p><p>gpcnet-network-test: Full system network tests in random and natural ring, alltoall and allreduce, at least 2 nodes</p><p>gpcnet-network-load-test: Select full system network tests run with four congestors to measure network congestion or contention, at least 10 nodes</p><ul><li>supporting network tests: RR Two-sided Lat (8 B), RR Get Lat (8 B), RR Two-sided BW (131072 B), RR Put BW (131072 B), RR Two-sided BW+Sync (131072 B), Nat Two-sided BW (131072 B), Multiple Allreduce (8 B), Multiple Alltoall (4096 B)</li><li>supporting congestors: Alltoall (4096 B), Two-sided Incast (4096 B), Put Incast (4096 B), Get Bcast (4096 B)</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-14"></a>Metrics<a class="hash-link" href="#metrics-14" title="Direct link to heading">#</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>gpcnet-network-test/rr<em>two-sided_lat</em>${stat}</td><td>time (us)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;random ring communication pattern two-side latency&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>two-sided+sync_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>fstatistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;random ring communication pattern two-side bandwidth with barrier&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/multiple<em>allreduce_time</em>${stat}</td><td>time (us)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;multiple allreduce bandwidth&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>get_lat</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;RR GetLat (8 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/rr<em>two-sided_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;RR Two-sidedBW (131072 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/nat<em>two-sided_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;Nat Two-sidedBW (131072 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-test/multiple<em>alltoall_bw</em>${stat}</td><td>bandwidth (MiB/s/rank)</td><td>statistical values(min, max, avg, 99%, 99.9%) obtained by all nodes use algorithm &#x27;Multiple Alltoall (4096 B)&#x27; for network testing</td></tr><tr><td>gpcnet-network-load-test/rr<em>two-sided_lat_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr><tr><td>gpcnet-network-load-test/rr<em>two-sided+sync_bw_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr><tr><td>gpcnet-network-load-test/multiple<em>allreduce_x</em>${stat}</td><td>factor (x)</td><td>summary about congestion impact factor of the network test algorithm</td></tr></tbody></table><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="ib-traffic"></a><code>ib-traffic</code><a class="hash-link" href="#ib-traffic" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-14"></a>Introduction<a class="hash-link" href="#introduction-14" title="Direct link to heading">#</a></h4><p>Measure the InfiniBand performance under multi nodes&#x27; traffic pattern.</p><p>The traffic pattern is defined in a config file, which is pre-defined for one-to-many, many-to-one and all-to-all patterns.
Each row in the config is one round, and all pairs of nodes in a row run ib command simultaneously.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-15"></a>Metrics<a class="hash-link" href="#metrics-15" title="Direct link to heading">#</a></h4><table><thead><tr><th>Metrics</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>ib-traffic/${command}<em>${line}</em>${pair}<em>${server}</em>${client}_bw</td><td>bandwidth (GB/s)</td><td>The max bandwidth of ib command (ib_write_bw, ib_send_bw, ib_read_bw) run between the ${pair}<sup>th</sup> node pair in the ${line}<sup>th</sup> line of the config, ${server} and ${client} are the hostname of server and client</td></tr><tr><td>ib-traffic/${command}<em>${line}</em>${pair}<em>${server}</em>${client}_lat</td><td>time (us)</td><td>The max latency of ib command (ib_write_lat, ib_send_lat, ib_read_lat) run between the ${pair}<sup>th</sup> node pair in the ${line}<sup>th</sup> line of the config, ${server} and ${client} are the hostname of server and client</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="computation-communication-benchmarks"></a>Computation-communication Benchmarks<a class="hash-link" href="#computation-communication-benchmarks" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="computation-communication-overlap"></a><code>computation-communication-overlap</code><a class="hash-link" href="#computation-communication-overlap" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-15"></a>Introduction<a class="hash-link" href="#introduction-15" title="Direct link to heading">#</a></h4><p>Test the performance of single node when communication and computation overlap.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-16"></a>Metrics<a class="hash-link" href="#metrics-16" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-computation-communication-overlap/mul_time</td><td>time (ms)</td><td>Time of communication and mul kernel computation overlap.</td></tr><tr><td>pytorch-computation-communication-overlap/matmul_time</td><td>time (ms)</td><td>Time of communication and matmul kernel computation overlap.</td></tr></tbody></table><h4></h4><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="sharding-matmul"></a><code>sharding-matmul</code><a class="hash-link" href="#sharding-matmul" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-16"></a>Introduction<a class="hash-link" href="#introduction-16" title="Direct link to heading">#</a></h4><p>Test the performance of large scale matmul operation with multiple GPUs:</p><ul><li>allreduce: Each GPU will calculate part of the MM calculation, and use AllReduce to merge all data into one tensor.</li><li>allgather: Each GPU will calculate part of the MM calculation, and use AllGather + Concat to merge all data into one tensor.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-17"></a>Metrics<a class="hash-link" href="#metrics-17" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>pytorch-sharding-matmul/allreduce_time</td><td>time (ms)</td><td>Time of sharding matmul using allreduce.</td></tr><tr><td>pytorch-sharding-matmul/allgather_time</td><td>time (ms)</td><td>Time of sharding matmul using allgather.</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="storage-benchmarks"></a>Storage Benchmarks<a class="hash-link" href="#storage-benchmarks" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="disk-benchmark"></a><code>disk-benchmark</code><a class="hash-link" href="#disk-benchmark" title="Direct link to heading">#</a></h3><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-17"></a>Introduction<a class="hash-link" href="#introduction-17" title="Direct link to heading">#</a></h4><p>Measure the disk performance through <a href="https://github.com/axboe/fio/tree/0313e938c9c8bb37d71dade239f1f5326677b079" target="_blank" rel="noopener noreferrer">FIO</a>.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="metrics-18"></a>Metrics<a class="hash-link" href="#metrics-18" title="Direct link to heading">#</a></h4><table><thead><tr><th>Name</th><th>Unit</th><th>Description</th></tr></thead><tbody><tr><td>disk-benchmark/${disk_name}_rand_read_write_bs</td><td>size (bytes)</td><td>Disk random read write block size.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_iops</td><td>IOPS</td><td>Disk random read write read IOPS.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_95.0</td><td>time (ns)</td><td>Disk random read write read latency in 95.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.0</td><td>time (ns)</td><td>Disk random read write read latency in 99.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_read_lat_ns_99.9</td><td>time (ns)</td><td>Disk random read write read latency in 99.9 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_iops</td><td>IOPS</td><td>Disk random read write write IOPS.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_95.0</td><td>time (ns)</td><td>Disk random read write write latency in 95.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.0</td><td>time (ns)</td><td>Disk random read write write latency in 99.0 percentile.</td></tr><tr><td>disk-benchmark/${disk_name}_rand_read_write_write_lat_ns_99.9</td><td>time (ns)</td><td>Disk random read write write latency in 99.9 percentile.</td></tr></tbody></table></div><footer class="row docusaurus-mt-lg"><div class="col"><a href="https://github.com/microsoft/superbenchmark/edit/main/website/../docs/user-tutorial/benchmarks/micro-benchmarks.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_3DPF"></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/superbenchmark/docs/getting-started/run-superbench"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Run SuperBench</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/superbenchmark/docs/user-tutorial/benchmarks/model-benchmarks"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Model Benchmarks Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#computation-benchmarks" class="table-of-contents__link">Computation Benchmarks</a><ul><li><a href="#kernel-launch" class="table-of-contents__link"><code>kernel-launch</code></a></li><li><a href="#gemm-flops" class="table-of-contents__link"><code>gemm-flops</code></a></li><li><a href="#matmul" class="table-of-contents__link"><code>matmul</code></a></li><li><a href="#cublas-function" class="table-of-contents__link"><code>cublas-function</code></a></li><li><a href="#cudnn-function" class="table-of-contents__link"><code>cudnn-function</code></a></li><li><a href="#tensorrt-inference" class="table-of-contents__link"><code>tensorrt-inference</code></a></li><li><a href="#ort-inference" class="table-of-contents__link"><code>ort-inference</code></a></li><li><a href="#gpu-burn" class="table-of-contents__link"><code>gpu-burn</code></a></li></ul></li><li><a href="#communication-benchmarks" class="table-of-contents__link">Communication Benchmarks</a><ul><li><a href="#cpu-memory-bw-latency" class="table-of-contents__link"><code>cpu-memory-bw-latency</code></a></li><li><a href="#mem-bw" class="table-of-contents__link"><code>mem-bw</code></a></li><li><a href="#gpu-copy-bw" class="table-of-contents__link"><code>gpu-copy-bw</code></a></li><li><a href="#ib-loopback" class="table-of-contents__link"><code>ib-loopback</code></a></li><li><a href="#nccl-bw--rccl-bw" class="table-of-contents__link"><code>nccl-bw</code> / <code>rccl-bw</code></a></li><li><a href="#tcp-connectivity" class="table-of-contents__link"><code>tcp-connectivity</code></a></li><li><a href="#gpcnet-network-test--gpcnet-network-load-test" class="table-of-contents__link"><code>gpcnet-network-test</code> / <code>gpcnet-network-load-test</code></a></li><li><a href="#ib-traffic" class="table-of-contents__link"><code>ib-traffic</code></a></li></ul></li><li><a href="#computation-communication-benchmarks" class="table-of-contents__link">Computation-communication Benchmarks</a><ul><li><a href="#computation-communication-overlap" class="table-of-contents__link"><code>computation-communication-overlap</code></a></li><li><a href="#sharding-matmul" class="table-of-contents__link"><code>sharding-matmul</code></a></li></ul></li><li><a href="#storage-benchmarks" class="table-of-contents__link">Storage Benchmarks</a><ul><li><a href="#disk-benchmark" class="table-of-contents__link"><code>disk-benchmark</code></a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/superbenchmark/docs/introduction">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/superbenchmark/docs/getting-started/installation">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/superbenchmark/docs/cli">API</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/microsoft/superbenchmark/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">Issues</a></li><li class="footer__item"><a href="https://github.com/microsoft/superbenchmark/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discussion</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/superbenchmark/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/microsoft/superbenchmark" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 SuperBench. <br> Built with Docusaurus and hosted by GitHub.</div></div></div></footer></div>
<script src="/superbenchmark/assets/js/runtime~main.1636966a.js"></script>
<script src="/superbenchmark/assets/js/main.fa6a49aa.js"></script>
</body>
</html>
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GPU to GPU bandwidth", "results": {"gpu-to-gpu-bw:perf/write_by_dma_bw": 396.20, "gpu-to-gpu-bw:perf/write_by_sm_bw": 372.42}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GPU Memory bandwidth", "results": {"gpu-mem-read-write-bw:perf": 3050.78}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "CPU memory bandwidth and latency", "results": {"cpu-memory-bw-latency/mem_bandwidth_matrix_numa_0_0_bw": 251751.57, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_0_1_bw": 128436.47, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_1_0_bw": 128405.53, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_1_1_bw": 252301.06, "cpu-memory-bw-latency/mem_latency_matrix_numa_0_0_lat": 115.34, "cpu-memory-bw-latency/mem_latency_matrix_numa_0_1_lat": 195.21, "cpu-memory-bw-latency/mem_latency_matrix_numa_1_0_lat": 195.04, "cpu-memory-bw-latency/mem_latency_matrix_numa_1_1_lat": 115.68, "cpu-memory-bw-latency/mem_max_bandwidth_1_1_reads-writes_bw": 401568.03, "cpu-memory-bw-latency/mem_max_bandwidth_2_1_reads-writes_bw": 413216.38, "cpu-memory-bw-latency/mem_max_bandwidth_3_1_reads-writes_bw": 423379.88, "cpu-memory-bw-latency/mem_max_bandwidth_all_reads_bw": 505203.54, "cpu-memory-bw-latency/mem_max_bandwidth_stream-triad_like_bw": 419482.70}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "CUBLAS functions' performance", "results": {"cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_128_m_128_n_32_transa_0_transb_1_time": 61.97, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_128_m_64_n_32_transa_0_transb_1_time": 35.04, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_3_m_64_n_32_transa_0_transb_1_time": 8.01, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_64_m_128_n_32_transa_0_transb_1_time": 36.10, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_64_m_64_n_32_transa_1_transb_0_time": 20.41, "cublas-function/name_cublascgemm_k_32_m_2048_n_512_transa_1_transb_0_time": 10.72, "cublas-function/name_cublascgemm_k_32_m_512_n_2048_transa_1_transb_0_time": 10.67, "cublas-function/name_cublascgemm_k_32_m_512_n_512_transa_1_transb_0_time": 5.50, "cublas-function/name_cublascgemm_k_32_m_640_n_1280_transa_1_transb_0_time": 10.20, "cublas-function/name_cublascgemm_k_32_m_896_n_1792_transa_1_transb_0_time": 13.99, "cublas-function/name_cublasgemmex_datatype_float_k_1000_m_4000_n_224_transa_0_transb_0_use_tensor_core_false_time": 52.19, "cublas-function/name_cublasgemmex_datatype_float_k_4000_m_1000_n_224_transa_0_transb_0_use_tensor_core_false_time": 55.19, "cublas-function/name_cublasgemmex_datatype_half_k_1000_m_4000_n_224_transa_1_transb_0_use_tensor_core_true_time": 14.40, "cublas-function/name_cublasgemmex_datatype_half_k_4000_m_1000_n_224_transa_0_transb_0_use_tensor_core_false_time": 19.38, "cublas-function/name_cublasgemmstridedbatchedex_batchcount_160_datatype_half_k_224_m_64_n_224_transa_0_transb_0_use_tensor_core_true_time": 10.06, "cublas-function/name_cublasgemmstridedbatchedex_batchcount_160_datatype_half_k_64_m_224_n_224_transa_0_transb_0_use_tensor_core_true_time": 21.86, "cublas-function/name_cublassgemm_k_1024_m_1024_n_7168_transa_1_transb_0_time": 361.16, "cublas-function/name_cublassgemmstridedbatched_batchcount_160_k_224_m_64_n_224_transa_0_transb_0_time": 53.23, "cublas-function/name_cublassgemmstridedbatched_batchcount_512_k_224_m_64_n_224_transa_0_transb_0_time": 117.50}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GEMM performance - FP8(FP8e4m3 and FP8e5m2) Precision, CUBLAS LT", "results": {"cublaslt-gemm/fp8e4m3_0_1024_12288_1536_flops": 1069.57, "cublaslt-gemm/fp8e4m3_0_1024_4608_12288_flops": 1219.99, "cublaslt-gemm/fp8e4m3_0_128_12288_1536_flops": 362.67, "cublaslt-gemm/fp8e4m3_0_128_4608_12288_flops": 472.30, "cublaslt-gemm/fp8e4m3_0_16384_16384_16384_flops": 1505.06, "cublaslt-gemm/fp8e4m3_0_16_12288_1536_flops": 48.44, "cublaslt-gemm/fp8e4m3_0_16_4608_12288_flops": 61.46, "cublaslt-gemm/fp8e4m3_0_2048_12288_1536_flops": 1395.14, "cublaslt-gemm/fp8e4m3_0_2048_4608_12288_flops": 1382.95, "cublaslt-gemm/fp8e4m3_0_256_12288_1536_flops": 556.88, "cublaslt-gemm/fp8e4m3_0_256_4608_12288_flops": 701.28, "cublaslt-gemm/fp8e4m3_0_32_12288_1536_flops": 96.91, "cublaslt-gemm/fp8e4m3_0_32_4608_12288_flops": 121.67, "cublaslt-gemm/fp8e4m3_0_4096_4096_4096_flops": 1597.83, "cublaslt-gemm/fp8e4m3_0_512_12288_1536_flops": 745.87, "cublaslt-gemm/fp8e4m3_0_512_4608_12288_flops": 1019.71, "cublaslt-gemm/fp8e4m3_0_64_12288_1536_flops": 206.47, "cublaslt-gemm/fp8e4m3_0_64_4608_12288_flops": 241.20, "cublaslt-gemm/fp8e4m3_0_8192_8192_8192_flops": 1508.61, "cublaslt-gemm/fp8e5m2_0_1024_12288_1536_flops": 1069.58, "cublaslt-gemm/fp8e5m2_0_1024_4608_12288_flops": 1220.19, "cublaslt-gemm/fp8e5m2_0_128_12288_1536_flops": 362.59, "cublaslt-gemm/fp8e5m2_0_128_4608_12288_flops": 472.03, "cublaslt-gemm/fp8e5m2_0_16384_16384_16384_flops": 1504.12, "cublaslt-gemm/fp8e5m2_0_16_12288_1536_flops": 48.43, "cublaslt-gemm/fp8e5m2_0_16_4608_12288_flops": 61.44, "cublaslt-gemm/fp8e5m2_0_2048_12288_1536_flops": 1395.15, "cublaslt-gemm/fp8e5m2_0_2048_4608_12288_flops": 1382.95, "cublaslt-gemm/fp8e5m2_0_256_12288_1536_flops": 556.87, "cublaslt-gemm/fp8e5m2_0_256_4608_12288_flops": 701.18, "cublaslt-gemm/fp8e5m2_0_32_12288_1536_flops": 96.90, "cublaslt-gemm/fp8e5m2_0_32_4608_12288_flops": 121.67, "cublaslt-gemm/fp8e5m2_0_4096_4096_4096_flops": 1597.86, "cublaslt-gemm/fp8e5m2_0_512_12288_1536_flops": 745.81, "cublaslt-gemm/fp8e5m2_0_512_4608_12288_flops": 1019.60, "cublaslt-gemm/fp8e5m2_0_64_12288_1536_flops": 206.47, "cublaslt-gemm/fp8e5m2_0_64_4608_12288_flops": 241.19, "cublaslt-gemm/fp8e5m2_0_8192_8192_8192_flops": 1508.63}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "distributed inference throughput", "results": {"step_times": 1.69, "step_times_50": 1.68, "step_times_90": 1.70, "step_times_95": 1.71, "step_times_99": 1.76, "step_times_99.9": 2.30}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GPU gemm performance for bf16, fp16, fp16(tensor core), fp32, fp32(tensor core), fp64, fp64(tensor core), int8, int4", "results": {"gemm-flops/bf16_tc_flops": 476913.05, "gemm-flops/fp16_flops": 60634.15, "gemm-flops/fp16_tc_flops": 860890, "gemm-flops/fp32_flops": 45963.81, "gemm-flops/fp64_flops": 22948.82, "gemm-flops/fp64_tc_flops": 32496.96, "gemm-flops/int8_tc_iops": 913521.93, "gemm-flops/tf32_tc_flops": 242235.2}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GPU kernel launch time", "results": {"kernel_launch_event_time": 0.0049, "kernel_launch_wall_time": 0.0071}}  
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "PCIe bandwidth, GPU to CPU bandwidth, CPU to GPU bandwidth", "results": {"d2h_bw": 53.22, "h2d_bw": 55.41}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "InfiniBand bandwidth(loopback)", "results": {"ib_write_bw_8388608": 46.86}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI bert model training performance, bert-base", "results": {"fp16_throughput": 1515.18, "fp32_throughput": 856.18, "fp8_throughput": 1585.58}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI Bert model training performance, bert-large", "results": {"fp16_throughput": 594.55, "fp32_throughput": 301.22, "fp8_throughput": 617.81}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI densenet model training performance, densenet169", "results": {"fp16_throughput": 1126.68, "fp32_throughput": 911.21}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI densenet model training performance, densenet201", "results": {"fp16_throughput": 945.31, "fp32_throughput": 723.55}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI GPT model training performance, gpt2-large", "results": {"fp16_throughput": 173.53, "fp32_throughput": 96.59, "fp8_throughput": 173.82}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI GPT model training performance, gpt2-small", "results": {"fp16_throughput": 818.07, "fp32_throughput": 501.53, "fp8_throughput": 834.09}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI lstm model training performance", "results": {"fp16_step_time": 38.667009719915086, "fp16_throughput": 26006.099835959703, "fp32_step_time": 67.83910552181047, "fp32_throughput": 15085.325796283136}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI resnet model training performance, resnet50", "results": {"fp16_step_time": 167.81567426946154, "fp16_throughput": 2290.680211613529, "fp32_step_time": 223.3167001201815, "fp32_throughput": 1720.0540235719639}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI resnet model training performance, resnet101", "results": {"fp16_step_time": 253.607873261, "fp16_throughput": 1514.9059081895002, "fp32_step_time": 338.73275362167897, "fp32_throughput": 1133.8761397986932}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI resnet model training performance, resnet152", "results": {"fp16_step_time": 331.3762566200576, "fp16_throughput": 1159.5209983945726, "fp32_step_time": 465.31608875908637, "fp32_throughput": 828.593823835775}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI vgg model training performance, vgg11", "results": {"fp16_throughput": 2806.14, "fp32_throughput": 1929.16}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI vgg model training performance, vgg13", "results": {"fp16_throughput": 2041.05, "fp32_throughput": 1273.65}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI vgg model training performance, vgg16", "results": {"fp16_throughput": 1747.68, "fp32_throughput": 1093.93}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "AI vgg model training performance, vgg19", "results": {"fp16_throughput": 1577.86, "fp32_throughput": 944.81}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl allreduce performance (nvlink bandwidth)", "results": {"allreduce_1024_busbw": 0.08, "allreduce_1048576_busbw": 66.4, "allreduce_1073741824_busbw": 362.76, "allreduce_131072_busbw": 10.52, "allreduce_134217728_busbw": 343.27, "allreduce_16384_busbw": 1.44, "allreduce_16777216_busbw": 234.72, "allreduce_17179869184_busbw": 369.05, "allreduce_2048_busbw": 0.21, "allreduce_2097152_busbw": 90.01, "allreduce_2147483648_busbw": 365.18, "allreduce_262144_busbw": 20.73, "allreduce_268435456_busbw": 351.58, "allreduce_32768_busbw": 2.7, "allreduce_33554432_busbw": 265.59, "allreduce_4096_busbw": 0.39, "allreduce_4194304_busbw": 143.5, "allreduce_4294967296_busbw": 367.19, "allreduce_524288_busbw": 39.82, "allreduce_536870912_busbw": 358.56, "allreduce_65536_busbw": 5.32, "allreduce_67108864_busbw": 332.93, "allreduce_8192_busbw": 0.74, "allreduce_8388608_busbw": 191.84, "allreduce_8589934592_busbw": 368.4}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl alltoall performance (nvlink bandwidth)", "results": {"alltoall_1024_busbw": 0.08, "alltoall_1048576_busbw": 61.52, "alltoall_1073741824_busbw": 330.79, "alltoall_131072_busbw": 10.03, "alltoall_134217728_busbw": 297.31, "alltoall_16384_busbw": 1.67, "alltoall_16777216_busbw": 206.17, "alltoall_17179869184_busbw": 348.63, "alltoall_2048_busbw": 0.21, "alltoall_2097152_busbw": 98.71, "alltoall_2147483648_busbw": 343.06, "alltoall_262144_busbw": 18.89, "alltoall_268435456_busbw": 308.87, "alltoall_32768_busbw": 3.18, "alltoall_33554432_busbw": 246.97, "alltoall_4096_busbw": 0.43, "alltoall_4194304_busbw": 140.56, "alltoall_4294967296_busbw": 346.1, "alltoall_524288_busbw": 31.67, "alltoall_536870912_busbw": 314.21,"alltoall_65536_busbw": 5.72, "alltoall_67108864_busbw": 273.79, "alltoall_8192_busbw": 0.84, "alltoall_8388608_busbw": 182.32, "alltoall_8589934592_busbw": 348.62}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl broadcast performance (nvlink bandwidth)", "results": {"broadcast_1024_busbw": 0.09, "broadcast_1048576_busbw": 62.4, "broadcast_1073741824_busbw": 363.59, "broadcast_131072_busbw": 10.96, "broadcast_134217728_busbw": 340.59, "broadcast_16384_busbw": 1.62, "broadcast_16777216_busbw": 256.57, "broadcast_17179869184_busbw": 367.21, "broadcast_2048_busbw": 0.26, "broadcast_2097152_busbw": 101.72, "broadcast_2147483648_busbw": 365.06, "broadcast_262144_busbw": 21.74, "broadcast_268435456_busbw": 352.65, "broadcast_32768_busbw": 2.84, "broadcast_33554432_busbw": 296.88, "broadcast_4096_busbw": 0.47, "broadcast_4194304_busbw": 155.1, "broadcast_4294967296_busbw": 366.5, "broadcast_524288_busbw": 42.38, "broadcast_536870912_busbw": 360.58, "broadcast_65536_busbw": 5.59, "broadcast_67108864_busbw": 324.36, "broadcast_8192_busbw": 0.86, "broadcast_8388608_busbw": 210.18, "broadcast_8589934592_busbw": 367.02}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl reduce performance (nvlink bandwidth)", "results": {"reduce_1024_busbw": 0.09, "reduce_1048576_busbw": 63.93, "reduce_1073741824_busbw": 366, "reduce_131072_busbw": 10.7, "reduce_134217728_busbw": 345.63, "reduce_16384_busbw": 1.45, "reduce_16777216_busbw": 261.39, "reduce_17179869184_busbw": 369.47, "reduce_2048_busbw": 0.24, "reduce_2097152_busbw": 103.48, "reduce_2147483648_busbw": 367.63, "reduce_262144_busbw": 20.78, "reduce_268435456_busbw": 356.85, "reduce_32768_busbw": 2.75, "reduce_33554432_busbw": 301.05, "reduce_4096_busbw": 0.41, "reduce_4194304_busbw": 156.21, "reduce_4294967296_busbw": 368.88, "reduce_524288_busbw": 39.54, "reduce_536870912_busbw": 363.78, "reduce_65536_busbw": 5.36, "reduce_67108864_busbw": 331.43, "reduce_8192_busbw": 0.77, "reduce_8388608_busbw": 213.56, "reduce_8589934592_busbw": 369.27}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl reducescatter performance (bandwidth)", "results": {"reducescatter_1024_busbw": 0.08, "reducescatter_1048576_busbw": 63.32, "reducescatter_1073741824_busbw": 361.59, "reducescatter_131072_busbw": 10.6, "reducescatter_134217728_busbw": 343.02, "reducescatter_16384_busbw": 1.42, "reducescatter_16777216_busbw": 259.78, "reducescatter_17179869184_busbw": 363.63, "reducescatter_2048_busbw": 0.2, "reducescatter_2097152_busbw": 102.9, "reducescatter_2147483648_busbw": 363.91, "reducescatter_262144_busbw": 20.63, "reducescatter_268435456_busbw": 354.63, "reducescatter_32768_busbw": 2.69, "reducescatter_33554432_busbw": 299.83, "reducescatter_4096_busbw": 0.39, "reducescatter_4194304_busbw": 154.87, "reducescatter_4294967296_busbw": 363.3, "reducescatter_524288_busbw": 38.87, "reducescatter_536870912_busbw": 359.83, "reducescatter_65536_busbw": 5.28, "reducescatter_67108864_busbw": 328.49, "reducescatter_8192_busbw": 0.75, "reducescatter_8388608_busbw": 211.73, "reducescatter_8589934592_busbw": 363.32}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "nccl allgather performance (bandwidth)", "results": {"allgather_1024_busbw": 0.08, "allgather_1048576_busbw": 59.3, "allgather_1073741824_busbw": 340.45, "allgather_131072_busbw": 10.17, "allgather_134217728_busbw": 287.93, "allgather_16384_busbw": 1.72, "allgather_16777216_busbw": 191.94, "allgather_17179869184_busbw": 358.94, "allgather_2048_busbw": 0.21, "allgather_2097152_busbw": 94.88, "allgather_2147483648_busbw": 348.97, "allgather_262144_busbw": 19.47, "allgather_268435456_busbw": 299.71, "allgather_32768_busbw": 3.2, "allgather_33554432_busbw": 238.42, "allgather_4096_busbw": 0.45, "allgather_4194304_busbw": 134.34, "allgather_4294967296_busbw": 350.32, "allgather_524288_busbw": 30.61, "allgather_536870912_busbw": 305.45, "allgather_65536_busbw": 6.04, "allgather_67108864_busbw": 264.63, "allgather_8192_busbw": 0.87, "allgather_8388608_busbw": 170.34, "allgather_8589934592_busbw": 356.45}}
{"hardware": "H100 GPU", "sku": "NDv5", "workload": "GPT-175B Mimic", "results": {"inference_batch_size_1_latency": 153.83, "inference_batch_size_64_latency": 168.2, "inference_batch_size_2048_latency": 1011.74}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "GPU to GPU bandwidth", "results": {"gpu-to-gpu-bw:perf/write_by_dma_bw": 279.55, "gpu-to-gpu-bw:perf/write_by_sm_bw": 258.68}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "GPU kernel launch time", "results": {"kernel_launch_event_time": 0.0056, "kernel_launch_wall_time": 0.0094}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "GPU gemm performance for bf16, fp16, fp16(tensor core), fp32, fp32(tensor core), fp64, fp64(tensor core), int8, int4", "results": {"gemm-flops/bf16_tc_flops": 261516.05, "gemm-flops/fp16_flops": 33866.20, "gemm-flops/fp16_tc_flops": 275713.45, "gemm-flops/fp32_flops": 18354.25, "gemm-flops/fp64_flops": 9034.69, "gemm-flops/fp64_tc_flops": 18947.00, "gemm-flops/int4_tc_iops": 957360.78, "gemm-flops/int8_tc_iops": 467046.05, "gemm-flops/tf32_tc_flops": 127436.62}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "CUBLAS functions' performance", "results": {"cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_128_m_128_n_32_transa_0_transb_1_time": 112.31, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_128_m_64_n_32_transa_0_transb_1_time": 65.08, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_3_m_64_n_32_transa_0_transb_1_time": 12.17, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_64_m_128_n_32_transa_0_transb_1_time": 62.44, "cublas-function/name_cublascgemm3mstridedbatched_batchcount_544_k_64_m_64_n_32_transa_1_transb_0_time": 37.30, "cublas-function/name_cublascgemm_k_32_m_2048_n_512_transa_1_transb_0_time": 21.81, "cublas-function/name_cublascgemm_k_32_m_512_n_2048_transa_1_transb_0_time": 21.72, "cublas-function/name_cublascgemm_k_32_m_512_n_512_transa_1_transb_0_time": 11.34, "cublas-function/name_cublascgemm_k_32_m_640_n_1280_transa_1_transb_0_time": 18.44, "cublas-function/name_cublascgemm_k_32_m_896_n_1792_transa_1_transb_0_time": 32.43, "cublas-function/name_cublasgemmex_datatype_float_k_1000_m_4000_n_224_transa_0_transb_0_use_tensor_core_false_time": 18.74, "cublas-function/name_cublasgemmex_datatype_float_k_4000_m_1000_n_224_transa_0_transb_0_use_tensor_core_false_time": 23.13, "cublas-function/name_cublasgemmex_datatype_half_k_1000_m_4000_n_224_transa_1_transb_0_use_tensor_core_true_time": 125.64, "cublas-function/name_cublasgemmex_datatype_half_k_4000_m_1000_n_224_transa_0_transb_0_use_tensor_core_false_time": 127.97, "cublas-function/name_cublasgemmstridedbatchedex_batchcount_160_datatype_half_k_224_m_64_n_224_transa_0_transb_0_use_tensor_core_true_time": 140.38, "cublas-function/name_cublasgemmstridedbatchedex_batchcount_160_datatype_half_k_64_m_224_n_224_transa_0_transb_0_use_tensor_core_true_time": 96.21, "cublas-function/name_cublassgemm_k_1024_m_1024_n_7168_transa_1_transb_0_time": 898.59, "cublas-function/name_cublassgemmstridedbatched_batchcount_160_k_224_m_64_n_224_transa__0_transb_0_time": 140.31, "cublas-function/name_cublassgemmstridedbatched_batchcount_512_k_224_m_64_n_224_transa_0_transb_0_time": 436.28}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "PCIe bandwidth, GPU to CPU bandwidth, CPU to GPU bandwidth", "results": {"d2h_bw": 26.28, "h2d_bw": 26.10}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "InfiniBand bandwidth(loopback)", "results": {"ib_write_bw_8388608": 25.00}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "NCCL allreduce bandwidth (nvlink)", "results": {"allreduce_8589934592_busbw": 236.02}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI BERT model training performance, bert-base", "results": {"fp16_train_throughput": 387.07, "fp32_train_throughput": 293.79}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI BERT model training performance, bert-large", "results": {"fp16_train_throughput": 158.35, "fp32_train_throughput": 108.76}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI GPT model training performance, gpt2-large", "results": {"fp16_train_throughput": 42.70, "fp32_train_throughput": 29.98}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI GPT model training performance, gpt2-small", "results": {"fp16_train_throughput": 162.30, "fp32_train_throughput": 139.46}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI LSTM model training performance", "results": {"fp16_train_throughput": 8031.78, "fp32_train_throughput": 4578.64}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI ResNet model training performance, resnet101", "results": {"fp16_train_throughput": 809.63, "fp32_train_throughput": 554.47}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI ResNet model training performance, resnet152", "results": {"fp16_train_throughput": 579.24, "fp32_train_throughput": 394.86}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI ResNet model training performance, resnet50", "results": {"fp16_train_throughput": 1186.15, "fp32_train_throughput": 844.65}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI DenseNet model training performance, densenet169", "results": {"fp16_train_throughput": 217.92, "fp32_train_throughput": 210.83}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI DenseNet model training performance, densenet201", "results": {"fp16_train_throughput": 180.97, "fp32_train_throughput": 173.98}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI VGG model training performance, vgg11", "results": {"fp16_train_throughput": 1109.91, "fp32_train_throughput": 731.88}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI VGG model training performance, vgg13", "results": {"fp16_train_throughput": 863.39, "fp32_train_throughput": 562.95}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI VGG model training performance, vgg16", "results": {"fp16_train_throughput": 761.41, "fp32_train_throughput": 481.55}}
{"hardware": "A100 GPU", "sku": "NDmv4", "workload": "AI VGG model training performance, vgg19", "results": {"fp16_train_throughput": 676.68, "fp32_train_throughput": 422.63}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "GPU to GPU bandwidth", "results": {"gpu-to-gpu-bw:perf/write_by_dma_bw": 45.21, "gpu-to-gpu-bw:perf/write_by_sm_bw": 47.19}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "GPU Memory bandwidth", "results": {"gpu-mem-read-write-bw:perf": 4255.86}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "CPU memory bandwidth and latency", "results": {"cpu-memory-bw-latency/mem_bandwidth_matrix_numa_0_0_bw": 248485.4, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_0_1_bw": 126392.2, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_1_0_bw": 126450.5, "cpu-memory-bw-latency/mem_bandwidth_matrix_numa_1_1_bw": 248495.0, "cpu-memory-bw-latency/mem_latency_matrix_numa_0_0_lat": 117.0, "cpu-memory-bw-latency/mem_latency_matrix_numa_0_1_lat": 195.8, "cpu-memory-bw-latency/mem_latency_matrix_numa_1_0_lat": 195.7, "cpu-memory-bw-latency/mem_latency_matrix_numa_1_1_lat": 116.6}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "GPU gemm performance for bf16, fp16, fp16(xdlops), fp32, fp32(xdlops), fp64, fp64(xdlops), int8, int4", "results": {"gemm-flops/bf16_xdlops_flops": 486370, "gemm-flops/fp16_xdlops_flops": 565010, "gemm-flops/fp32_flops": 84755, "gemm-flops/fp64_flops": 65340, "gemm-flops/int8_xdlops_iops": 1016960, "gemm-flops/tf32_xdlops_flops": 98280}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "GPU kernel launch time", "results": {"kernel_launch_event_time": 0.005, "kernel_launch_wall_time": 0.00028}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "PCIe bandwidth, GPU to CPU bandwidth, CPU to GPU bandwidth", "results": {"d2h_bw": 55.9, "h2d_bw": 55.73}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "InfiniBand bandwidth(loopback)", "results": {"ib_write_bw_8388608": 47.16}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI bert model training performance, bert-base", "results": {"fp16_throughput": 538.66, "fp32_throughput": 279.0}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI Bert model training performance, bert-large", "results": {"fp16_throughput": 221.94, "fp32_throughput": 105.1}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI densenet model training performance, densenet169", "results": {"fp16_throughput": 630.81, "fp32_throughput": 515.11}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI densenet model training performance, densenet201", "results": {"fp16_throughput": 475.99, "fp32_throughput": 367.87}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI GPT model training performance, gpt2-large", "results": {"fp16_throughput": 79.15, "fp32_throughput": 39.75}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI GPT model training performance, gpt2-small", "results": {"fp16_throughput": 320.6, "fp32_throughput": 213.95}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI lstm model training performance", "results": {"fp16_throughput": 5734.26, "fp32_throughput": 3623.25}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI resnet model training performance, resnet50", "results": {"fp16_throughput": 1811.19, "fp32_throughput": 1167.88}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI resnet model training performance, resnet101", "results": {"fp16_throughput": 1027.34, "fp32_throughput": 740.57}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI resnet model training performance, resnet152", "results": {"fp16_throughput": 788.93, "fp32_throughput": 522.48}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI vgg model training performance, vgg11", "results": {"fp16_throughput": 2015.42, "fp32_throughput": 947.6}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI vgg model training performance, vgg13", "results": {"fp16_throughput": 1253.46, "fp32_throughput": 677.82}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI vgg model training performance, vgg16", "results": {"fp16_throughput": 1222.82, "fp32_throughput": 550.15}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "AI vgg model training performance, vgg19", "results": {"fp16_throughput": 1026.79, "fp32_throughput": 464.79}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "rccl allreduce performance (xGMI bandwidth)", "results": {"allreduce_8589934592_busbw": 222.44}}
{"hardware": "MI300x GPU", "sku": "MI300x pv1", "workload": "GPT-175B Mimic", "results": {"inference_batch_size_1_latency": 178.66, "inference_batch_size_64_latency": 310.5, "inference_batch_size_2048_latency": 2896.1}}